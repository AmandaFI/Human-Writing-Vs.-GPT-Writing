{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo\n",
    "\n",
    "- Função de janelamento \n",
    "    - por numero de frases (tpkenizar em frases primeiro)\n",
    "    - Analisar tamanho das noticias para definir tamanho/quantidade de frases por janela\n",
    "- Comapra tamanho das noticias reais com as geradas pelo gpt graficamente (settar range dos graficos para melhor comparação)\n",
    "- Extrair features  -- pensar em mais features, as vezes até extrair features das pos_tags\n",
    "- Normalizar featuers extraidas\n",
    "- Gerar dataset de features com e sem janelamento\n",
    "- Construir redes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Val Split - Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['173_BBC.txt', '211_BBC.txt', '153_BBC.txt', '100_BBC.txt', '49_BBC.txt', '106_BBC.txt', '39_BBC.txt', '174_BBC.txt', '195_BBC.txt', '196_BBC.txt', '34_BBC.txt', '160_BBC.txt', '184_BBC.txt', '168_BBC.txt', '222_BBC.txt', '36_BBC.txt', '73_BBC.txt', '20_BBC.txt', '238_BBC.txt', '122_BBC.txt', '198_BBC.txt', '107_BBC.txt', '35_BBC.txt', '133_BBC.txt', '225_BBC.txt', '105_BBC.txt', '90_BBC.txt', '237_BBC.txt', '58_BBC.txt', '179_BBC.txt', '6_BBC.txt', '178_BBC.txt', '146_BBC.txt', '64_BBC.txt', '206_BBC.txt', '87_BBC.txt', '92_BBC.txt', '119_BBC.txt', '157_BBC.txt', '10_BBC.txt', '224_BBC.txt', '227_BBC.txt', '218_BBC.txt', '85_BBC.txt', '129_BBC.txt', '101_BBC.txt', '19_BBC.txt', '167_BBC.txt', '152_BBC.txt', '172_BBC.txt', '240_BBC.txt', '169_BBC.txt', '212_BBC.txt', '53_BBC.txt', '141_BBC.txt', '203_BBC.txt', '213_BBC.txt', '98_BBC.txt', '81_BBC.txt', '97_BBC.txt', '228_BBC.txt', '27_BBC.txt', '189_BBC.txt', '205_BBC.txt', '30_BBC.txt', '44_BBC.txt', '155_BBC.txt', '239_BBC.txt', '230_BBC.txt', '17_BBC.txt', '171_BBC.txt', '181_BBC.txt', '219_BBC.txt', '24_BBC.txt', '89_BBC.txt', '215_BBC.txt', '60_BBC.txt', '71_BBC.txt', '111_BBC.txt', '116_BBC.txt', '51_BBC.txt', '26_BBC.txt', '77_BBC.txt', '200_BBC.txt', '94_BBC.txt', '199_BBC.txt', '158_BBC.txt', '33_BBC.txt', '151_BBC.txt', '25_BBC.txt', '1_BBC.txt', '48_BBC.txt', '229_BBC.txt', '182_BBC.txt', '7_BBC.txt', '12_BBC.txt', '197_BBC.txt', '79_BBC.txt', '61_BBC.txt', '28_BBC.txt', '54_BBC.txt', '70_BBC.txt', '176_BBC.txt', '147_BBC.txt', '187_BBC.txt', '99_BBC.txt', '103_BBC.txt', '209_BBC.txt', '13_BBC.txt', '170_BBC.txt', '11_BBC.txt', '50_BBC.txt', '55_BBC.txt', '175_BBC.txt', '232_BBC.txt', '193_BBC.txt', '112_BBC.txt', '113_BBC.txt', '78_BBC.txt', '32_BBC.txt', '109_BBC.txt', '149_BBC.txt', '40_BBC.txt', '75_BBC.txt', '234_BBC.txt', '165_BBC.txt', '96_BBC.txt', '47_BBC.txt', '190_BBC.txt', '41_BBC.txt', '21_BBC.txt', '63_BBC.txt', '191_BBC.txt', '46_BBC.txt', '221_BBC.txt', '163_BBC.txt', '37_BBC.txt', '16_BBC.txt', '117_BBC.txt', '128_BBC.txt', '74_BBC.txt', '166_BBC.txt']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ BBC\n",
    "bbc_news_files = os.listdir('BBC/human_BBC_news/')\n",
    "bbc_labels = np.zeros(len(bbc_news_files))\n",
    "\n",
    "# X_bbc_train_files, X_bbc_test_files, Y_bbc_train, Y_bbc_test =  train_test_split(bbc_news_files, bbc_labels, test_size=0.4)\n",
    "X_bbc_train_files, X_bbc_test_files, Y_bbc_train, Y_bbc_test =  train_test_split(bbc_news_files, bbc_labels, test_size=0.3)\n",
    "\n",
    "print(X_bbc_train_files)\n",
    "\n",
    "X_bbc_val_files = X_bbc_test_files[:len(X_bbc_test_files)//2]\n",
    "X_bbc_test_files = X_bbc_test_files[len(X_bbc_test_files)//2:]\n",
    "Y_bbc_val = Y_bbc_test[:len(Y_bbc_test)//2]\n",
    "Y_bbc_test = Y_bbc_test[len(Y_bbc_test)//2:]\n",
    "\n",
    "# ------------------------------------ TechCrunch\n",
    "techCrunch_news_files = os.listdir('TechCrunch/human_TechCrunch_news/')\n",
    "techCrunch_labels = np.zeros(len(techCrunch_news_files))\n",
    "\n",
    "# X_techCrunch_train_files, X_techCrunch_test_files, Y_techCrunch_train, Y_techCrunch_test =  train_test_split(techCrunch_news_files, techCrunch_labels, test_size=0.4)\n",
    "X_techCrunch_train_files, X_techCrunch_test_files, Y_techCrunch_train, Y_techCrunch_test =  train_test_split(techCrunch_news_files, techCrunch_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "X_techCrunch_val_files = X_techCrunch_test_files[:len(X_techCrunch_test_files)//2]\n",
    "X_techCrunch_test_files = X_techCrunch_test_files[len(X_techCrunch_test_files)//2:]\n",
    "Y_techCrunch_val = Y_techCrunch_test[:len(Y_techCrunch_test)//2]\n",
    "Y_techCrunch_test = Y_techCrunch_test[len(Y_techCrunch_test)//2:]\n",
    "\n",
    "# ------------------------------------ TheVerge\n",
    "theVerge_news_files = os.listdir('TheVerge/human_TheVerge_news/')\n",
    "theVerge_labels = np.zeros(len(theVerge_news_files))\n",
    "\n",
    "# X_theVerge_train_files, X_theVerge_test_files, Y_theVerge_train, Y_theVerge_test =  train_test_split(theVerge_news_files, theVerge_labels, test_size=0.4)\n",
    "X_theVerge_train_files, X_theVerge_test_files, Y_theVerge_train, Y_theVerge_test =  train_test_split(theVerge_news_files, theVerge_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "X_theVerge_val_files = X_theVerge_test_files[:len(X_theVerge_test_files)//2]\n",
    "X_theVerge_test_files = X_theVerge_test_files[len(X_theVerge_test_files)//2:]\n",
    "Y_theVerge_val = Y_theVerge_test[:len(Y_theVerge_test)//2]\n",
    "Y_theVerge_test = Y_theVerge_test[len(Y_theVerge_test)//2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC --> train_files: 142, train_labels: 142, test_files: 31, test_labels: 31, val_files: 30, val_labels: 30\n",
      "Tech Crunch --> train_files: 176, train_labels: 176, test_files: 38, test_labels: 38, val_files: 38, val_labels: 38\n",
      "The Verge --> train_files: 117, train_labels: 117, test_files: 26, test_labels: 26, val_files: 25, val_labels: 25\n"
     ]
    }
   ],
   "source": [
    "print(f'BBC --> train_files: {len(X_bbc_train_files)}, train_labels: {len(Y_bbc_train)}, test_files: {len(X_bbc_test_files)}, test_labels: {len(Y_bbc_test)}, val_files: {len(X_bbc_val_files)}, val_labels: {len(Y_bbc_val)}')\n",
    "print(f'Tech Crunch --> train_files: {len(X_techCrunch_train_files)}, train_labels: {len(Y_techCrunch_train)}, test_files: {len(X_techCrunch_test_files)}, test_labels: {len(Y_techCrunch_test)}, val_files: {len(X_techCrunch_val_files)}, val_labels: {len(Y_techCrunch_val)}')\n",
    "print(f'The Verge --> train_files: {len(X_theVerge_train_files)}, train_labels: {len(Y_theVerge_train)}, test_files: {len(X_theVerge_test_files)}, test_labels: {len(Y_theVerge_test)}, val_files: {len(X_theVerge_val_files)}, val_labels: {len(Y_theVerge_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Humam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    content = None\n",
    "    folder = 'BBC/human_BBC_news/' if 'BBC' in file else 'TechCrunch/human_TechCrunch_news/' if 'TechCrunch' in file else 'TheVerge/human_TheVerge_news/'\n",
    "    with open(f'{folder}{file}', 'r', encoding=\"utf-8\") as news:\n",
    "        content = news.read()\n",
    "    return content\n",
    "\n",
    "X_bbc_train = np.array(list(map(load_data, X_bbc_train_files)))\n",
    "X_bbc_test = np.array(list(map(load_data, X_bbc_test_files)))\n",
    "X_bbc_val = np.array(list(map(load_data, X_bbc_val_files)))\n",
    "\n",
    "X_techCrunch_train = np.array(list(map(load_data, X_techCrunch_train_files)))\n",
    "X_techCrunch_test = np.array(list(map(load_data, X_techCrunch_test_files)))\n",
    "X_techCrunch_val = np.array(list(map(load_data, X_techCrunch_val_files)))\n",
    "\n",
    "X_theVerge_train = np.array(list(map(load_data, X_theVerge_train_files)))\n",
    "X_theVerge_test = np.array(list(map(load_data, X_theVerge_test_files)))\n",
    "X_theVerge_val = np.array(list(map(load_data, X_theVerge_val_files)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Humam Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human train: 435, 435\n",
      "Human test: 95, 95\n",
      "Human val: 93, 93\n"
     ]
    }
   ],
   "source": [
    "X_human_train = np.hstack((X_bbc_train, X_techCrunch_train, X_theVerge_train))\n",
    "Y_human_train = np.hstack((Y_bbc_train, Y_techCrunch_train, Y_theVerge_train))\n",
    "print(f'Human train: {len(X_human_train)}, {len(Y_human_train)}')\n",
    "\n",
    "X_human_test = np.hstack((X_bbc_test, X_techCrunch_test, X_theVerge_test))\n",
    "Y_human_test = np.hstack((Y_bbc_test, Y_techCrunch_test, Y_theVerge_test))\n",
    "print(f'Human test: {len(X_human_test)}, {len(Y_human_test)}')\n",
    "\n",
    "X_human_val = np.hstack((X_bbc_val, X_techCrunch_val, X_theVerge_val))\n",
    "Y_human_val = np.hstack((Y_bbc_val, Y_techCrunch_val, Y_theVerge_val))\n",
    "print(f'Human val: {len(X_human_val)}, {len(Y_human_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Last month a controversial facial recognition company, Clearview AI, announced it had given its technology to the Ukrainian government. The BBC has been given evidence of how it is being used - in more than a thousand cases - to identify both the living and the dead.  This story contains graphic descriptions that may be upsetting to some readers. A man lies motionless on the floor, his head tilted down. His body is naked, apart from a pair of Calvin Klein boxers. His eyes are ringed with what look like bruises. The body was found in Kharkiv, eastern Ukraine - in the wreckage of war. The BBC has seen pictures taken at the scene, but does not know the circumstances around his death. There is clear evidence of head trauma. He also had a tattoo on his left shoulder.  Ukrainian authorities didn't know who the man was, so dec\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_human_train[0].split('\\\\')\n",
    "X_human_train[0][:831]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle\n",
    "\n",
    "human_train= [X_human_train, X_human_test, X_human_val]\n",
    "\n",
    "filename = 'data_pickles/train_test_val_Human'\n",
    "file = open(filename, 'wb')\n",
    "pickle.dump(human_train, file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Human PosTags Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data(file):\n",
    "#     content = None\n",
    "#     folder = 'BBC/human_BBC_posTags/' if 'BBC' in file else 'TechCrunch/human_TechCrunch_posTags/' if 'TechCrunch' in file else 'TheVerge/human_TheVerge_posTags/'\n",
    "#     with open(f'{folder}{file[:-4]}_pos_tag.txt', 'r', encoding=\"utf-8\") as news:\n",
    "#         content = news.read()\n",
    "#         posTags_tokens = nltk.word_tokenize(content)\n",
    "#     return posTags_tokens\n",
    "\n",
    "def load_data(file):\n",
    "    content = None\n",
    "    pos_tags = None\n",
    "    folder = 'BBC/human_BBC_news/' if 'BBC' in file else 'TechCrunch/human_TechCrunch_news/' if 'TechCrunch' in file else 'TheVerge/human_TheVerge_news/'\n",
    "    with open(f'{folder}{file}', 'r', encoding=\"utf-8\") as news:\n",
    "        content = news.read()\n",
    "\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        pos_tag_tuples = nltk.pos_tag(tokens, lang='eng')\n",
    "        pos_tags = [tag[1] for tag in pos_tag_tuples]\n",
    "        \n",
    "    return pos_tags\n",
    "\n",
    "# print(load_data(X_bbc_train_files[0]))\n",
    "\n",
    "X_bbc_human_posTags_train = np.array(list(map(load_data, X_bbc_train_files)))\n",
    "X_bbc_human_posTags_test = np.array(list(map(load_data, X_bbc_test_files)))\n",
    "X_bbc_human_posTags_val = np.array(list(map(load_data, X_bbc_val_files)))\n",
    "\n",
    "X_techCrunch_human_posTags_train = np.array(list(map(load_data, X_techCrunch_train_files)))\n",
    "X_techCrunch_human_posTags_test = np.array(list(map(load_data, X_techCrunch_test_files)))\n",
    "X_techCrunch_human_posTags_val = np.array(list(map(load_data, X_techCrunch_val_files)))\n",
    "\n",
    "X_theVerge_human_posTags_train = np.array(list(map(load_data, X_theVerge_train_files)))\n",
    "X_theVerge_human_posTags_test = np.array(list(map(load_data, X_theVerge_test_files)))\n",
    "X_theVerge_human_posTags_val = np.array(list(map(load_data, X_theVerge_val_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bbc_train_files[0], X_bbc_human_posTags_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human train: 435\n",
      "Human test: 95\n",
      "Human val: 93\n"
     ]
    }
   ],
   "source": [
    "X_human_train_posTags = np.hstack((X_bbc_human_posTags_train, X_techCrunch_human_posTags_train, X_theVerge_human_posTags_train))\n",
    "print(f'Human train: {len(X_human_train_posTags)}')\n",
    "\n",
    "X_human_test_posTags = np.hstack((X_bbc_human_posTags_test, X_techCrunch_human_posTags_test, X_theVerge_human_posTags_test))\n",
    "print(f'Human test: {len(X_human_test_posTags)}')\n",
    "\n",
    "X_human_val_posTags = np.hstack((X_bbc_human_posTags_val, X_techCrunch_human_posTags_val, X_theVerge_human_posTags_val))\n",
    "print(f'Human val: {len(X_human_val_posTags)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle\n",
    "\n",
    "human_posTags= [X_human_train_posTags, X_human_test_posTags, X_human_val_posTags]\n",
    "\n",
    "filename = 'data_pickles/train_test_val_Human_posTags'\n",
    "file = open(filename, 'wb')\n",
    "pickle.dump(human_posTags, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT Data\n",
    "#### Randomizing which set (Train, Test or Val) the humam and GPT version of a same news will be placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['205_BBC_gpt.txt', '71_BBC_gpt.txt', '98_BBC_gpt.txt', '66_BBC_gpt.txt', '166_BBC_gpt.txt', '204_BBC_gpt.txt', '225_BBC_gpt.txt', '9_BBC_gpt.txt', '16_BBC_gpt.txt', '227_BBC_gpt.txt', '183_BBC_gpt.txt', '111_BBC_gpt.txt', '46_BBC_gpt.txt', '216_BBC_gpt.txt', '192_BBC_gpt.txt', '83_BBC_gpt.txt', '30_BBC_gpt.txt', '217_BBC_gpt.txt', '228_BBC_gpt.txt', '109_BBC_gpt.txt', '65_BBC_gpt.txt', '237_BBC_gpt.txt', '158_BBC_gpt.txt', '86_BBC_gpt.txt', '219_BBC_gpt.txt', '165_BBC_gpt.txt', '156_BBC_gpt.txt', '47_BBC_gpt.txt', '44_BBC_gpt.txt', '179_BBC_gpt.txt', '211_BBC_gpt.txt', '62_BBC_gpt.txt', '26_BBC_gpt.txt', '170_BBC_gpt.txt', '129_BBC_gpt.txt', '155_BBC_gpt.txt', '148_BBC_gpt.txt', '3_BBC_gpt.txt', '19_BBC_gpt.txt', '112_BBC_gpt.txt', '57_BBC_gpt.txt', '27_BBC_gpt.txt', '41_BBC_gpt.txt', '196_BBC_gpt.txt', '157_BBC_gpt.txt', '209_BBC_gpt.txt', '7_BBC_gpt.txt', '226_BBC_gpt.txt', '6_BBC_gpt.txt', '51_BBC_gpt.txt', '147_BBC_gpt.txt', '221_BBC_gpt.txt', '37_BBC_gpt.txt', '153_BBC_gpt.txt', '178_BBC_gpt.txt', '160_BBC_gpt.txt', '220_BBC_gpt.txt', '68_BBC_gpt.txt', '241_BBC_gpt.txt', '107_BBC_gpt.txt', '72_BBC_gpt.txt', '79_BBC_gpt.txt', '238_BBC_gpt.txt', '182_BBC_gpt.txt', '78_BBC_gpt.txt', '176_BBC_gpt.txt', '240_BBC_gpt.txt', '56_BBC_gpt.txt', '195_BBC_gpt.txt', '20_BBC_gpt.txt', '104_BBC_gpt.txt', '193_BBC_gpt.txt', '1_BBC_gpt.txt', '55_BBC_gpt.txt', '35_BBC_gpt.txt', '173_BBC_gpt.txt', '235_BBC_gpt.txt', '52_BBC_gpt.txt', '63_BBC_gpt.txt', '187_BBC_gpt.txt', '67_BBC_gpt.txt', '202_BBC_gpt.txt', '190_BBC_gpt.txt', '8_BBC_gpt.txt', '199_BBC_gpt.txt', '17_BBC_gpt.txt', '145_BBC_gpt.txt', '89_BBC_gpt.txt', '21_BBC_gpt.txt', '73_BBC_gpt.txt', '208_BBC_gpt.txt', '15_BBC_gpt.txt', '61_BBC_gpt.txt', '12_BBC_gpt.txt', '231_BBC_gpt.txt', '168_BBC_gpt.txt', '4_BBC_gpt.txt', '10_BBC_gpt.txt', '96_BBC_gpt.txt', '53_BBC_gpt.txt', '75_BBC_gpt.txt', '97_BBC_gpt.txt', '11_BBC_gpt.txt', '50_BBC_gpt.txt', '28_BBC_gpt.txt', '128_BBC_gpt.txt', '234_BBC_gpt.txt', '74_BBC_gpt.txt', '77_BBC_gpt.txt', '106_BBC_gpt.txt', '105_BBC_gpt.txt', '64_BBC_gpt.txt', '230_BBC_gpt.txt', '88_BBC_gpt.txt', '189_BBC_gpt.txt', '90_BBC_gpt.txt', '152_BBC_gpt.txt', '103_BBC_gpt.txt', '38_BBC_gpt.txt', '120_BBC_gpt.txt', '141_BBC_gpt.txt', '188_BBC_gpt.txt', '87_BBC_gpt.txt', '143_BBC_gpt.txt', '200_BBC_gpt.txt', '34_BBC_gpt.txt', '149_BBC_gpt.txt', '101_BBC_gpt.txt', '42_BBC_gpt.txt', '58_BBC_gpt.txt', '215_BBC_gpt.txt', '93_BBC_gpt.txt', '210_BBC_gpt.txt', '127_BBC_gpt.txt', '33_BBC_gpt.txt', '212_BBC_gpt.txt', '223_BBC_gpt.txt', '113_BBC_gpt.txt', '197_BBC_gpt.txt', '213_BBC_gpt.txt', '167_BBC_gpt.txt', '99_BBC_gpt.txt']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ BBC\n",
    "bbc_news_GPT_files = os.listdir('BBC/GPT_BBC_news/')\n",
    "bbc_GPT_labels = np.ones(len(bbc_news_GPT_files))\n",
    "\n",
    "# X_bbc_GPT_train_files, X_bbc_GPT_test_files, Y_bbc_GPT_train, Y_bbc_GPT_test =  train_test_split(bbc_news_GPT_files, bbc_GPT_labels, test_size=0.4)\n",
    "X_bbc_GPT_train_files, X_bbc_GPT_test_files, Y_bbc_GPT_train, Y_bbc_GPT_test =  train_test_split(bbc_news_GPT_files, bbc_GPT_labels, test_size=0.3)\n",
    "\n",
    "print(X_bbc_GPT_train_files)\n",
    "\n",
    "X_bbc_GPT_val_files = X_bbc_GPT_test_files[:len(X_bbc_GPT_test_files)//2]\n",
    "X_bbc_GPT_test_files = X_bbc_GPT_test_files[len(X_bbc_GPT_test_files)//2:]\n",
    "Y_bbc_GPT_val = Y_bbc_GPT_test[:len(Y_bbc_GPT_test)//2]\n",
    "Y_bbc_GPT_test = Y_bbc_GPT_test[len(Y_bbc_GPT_test)//2:]\n",
    "\n",
    "# ------------------------------------ TechCrunch\n",
    "techCrunch_news_GPT_files = os.listdir('TechCrunch/GPT_TechCrunch_news/')\n",
    "techCrunch_GPT_labels = np.ones(len(techCrunch_news_GPT_files))\n",
    "\n",
    "# X_techCrunch_GPT_train_files, X_techCrunch_GPT_test_files, Y_techCrunch_GPT_train, Y_techCrunch_GPT_test =  train_test_split(techCrunch_news_GPT_files, techCrunch_GPT_labels, test_size=0.4)\n",
    "X_techCrunch_GPT_train_files, X_techCrunch_GPT_test_files, Y_techCrunch_GPT_train, Y_techCrunch_GPT_test =  train_test_split(techCrunch_news_GPT_files, techCrunch_GPT_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "X_techCrunch_GPT_val_files = X_techCrunch_GPT_test_files[:len(X_techCrunch_GPT_test_files)//2]\n",
    "X_techCrunch_GPT_test_files = X_techCrunch_GPT_test_files[len(X_techCrunch_GPT_test_files)//2:]\n",
    "Y_techCrunch_GPT_val = Y_techCrunch_GPT_test[:len(Y_techCrunch_GPT_test)//2]\n",
    "Y_techCrunch_GPT_test = Y_techCrunch_GPT_test[len(Y_techCrunch_GPT_test)//2:]\n",
    "\n",
    "# ------------------------------------ TheVerge\n",
    "theVerge_news_GPT_files = os.listdir('TheVerge/GPT_TheVerge_news/')\n",
    "theVerge_GPT_labels = np.ones(len(theVerge_news_GPT_files))\n",
    "\n",
    "# X_theVerge_GPT_train_files, X_theVerge_GPT_test_files, Y_theVerge_GPT_train, Y_theVerge_GPT_test =  train_test_split(theVerge_news_GPT_files, theVerge_GPT_labels, test_size=0.4)\n",
    "X_theVerge_GPT_train_files, X_theVerge_GPT_test_files, Y_theVerge_GPT_train, Y_theVerge_GPT_test =  train_test_split(theVerge_news_GPT_files, theVerge_GPT_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "X_theVerge_GPT_val_files = X_theVerge_GPT_test_files[:len(X_theVerge_GPT_test_files)//2]\n",
    "X_theVerge_GPT_test_files = X_theVerge_GPT_test_files[len(X_theVerge_GPT_test_files)//2:]\n",
    "Y_theVerge_GPT_val = Y_theVerge_GPT_test[:len(Y_theVerge_GPT_test)//2]\n",
    "Y_theVerge_GPT_test = Y_theVerge_GPT_test[len(Y_theVerge_GPT_test)//2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC GPT --> train_files: 142, train_labels: 142, test_files: 31, test_labels: 31, val_files: 30, val_labels: 30\n",
      "Tech Crunch GPT --> train_files: 176, train_labels: 176, test_files: 38, test_labels: 38, val_files: 38, val_labels: 38\n",
      "The Verge GPT--> train_files: 117, train_labels: 117, test_files: 26, test_labels: 26, val_files: 25, val_labels: 25\n"
     ]
    }
   ],
   "source": [
    "print(f'BBC GPT --> train_files: {len(X_bbc_GPT_train_files)}, train_labels: {len(Y_bbc_GPT_train)}, test_files: {len(X_bbc_GPT_test_files)}, test_labels: {len(Y_bbc_GPT_test)}, val_files: {len(X_bbc_GPT_val_files)}, val_labels: {len(Y_bbc_GPT_val)}')\n",
    "print(f'Tech Crunch GPT --> train_files: {len(X_techCrunch_GPT_train_files)}, train_labels: {len(Y_techCrunch_GPT_train)}, test_files: {len(X_techCrunch_GPT_test_files)}, test_labels: {len(Y_techCrunch_GPT_test)}, val_files: {len(X_techCrunch_GPT_val_files)}, val_labels: {len(Y_techCrunch_GPT_val)}')\n",
    "print(f'The Verge GPT--> train_files: {len(X_theVerge_GPT_train_files)}, train_labels: {len(Y_theVerge_GPT_train)}, test_files: {len(X_theVerge_GPT_test_files)}, test_labels: {len(Y_theVerge_GPT_test)}, val_files: {len(X_theVerge_GPT_val_files)}, val_labels: {len(Y_theVerge_GPT_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    content = None\n",
    "    folder = 'BBC/GPT_BBC_news/' if 'BBC' in file else 'TechCrunch/GPT_TechCrunch_news/' if 'TechCrunch' in file else 'TheVerge/GPT_TheVerge_news/'\n",
    "    with open(f'{folder}{file}', 'r', encoding=\"utf-8\") as news:\n",
    "        content = news.read()\n",
    "    return content\n",
    "\n",
    "X_bbc_gpt_train = np.array(list(map(load_data, X_bbc_GPT_train_files)))\n",
    "X_bbc_gpt_test = np.array(list(map(load_data, X_bbc_GPT_test_files)))\n",
    "X_bbc_gpt_val = np.array(list(map(load_data, X_bbc_GPT_val_files)))\n",
    "\n",
    "X_techCrunch_gpt_train = np.array(list(map(load_data, X_techCrunch_GPT_train_files)))\n",
    "X_techCrunch_gpt_test = np.array(list(map(load_data, X_techCrunch_GPT_test_files)))\n",
    "X_techCrunch_gpt_val = np.array(list(map(load_data, X_techCrunch_GPT_val_files)))\n",
    "\n",
    "X_theVerge_gpt_train = np.array(list(map(load_data, X_theVerge_GPT_train_files)))\n",
    "X_theVerge_gpt_test = np.array(list(map(load_data, X_theVerge_GPT_test_files)))\n",
    "X_theVerge_gpt_val = np.array(list(map(load_data, X_theVerge_GPT_val_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT Data\n",
    "#### Keeping humam and GPT version of a same news in the same set (Train, Test or Val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    content = None\n",
    "    folder = 'BBC/GPT_BBC_news/' if 'BBC' in file else 'TechCrunch/GPT_TechCrunch_news/' if 'TechCrunch' in file else 'TheVerge/GPT_TheVerge_news/'\n",
    "    with open(f'{folder}{file[:-4]}_gpt.txt', 'r', encoding=\"utf-8\") as news:\n",
    "        content = news.read()\n",
    "        re.sub(\"\\n\\n\", \" \", content)\n",
    "    return content\n",
    "\n",
    "X_bbc_gpt_train = np.array(list(map(load_data, X_bbc_train_files)))\n",
    "X_bbc_gpt_test = np.array(list(map(load_data, X_bbc_test_files)))\n",
    "X_bbc_gpt_val = np.array(list(map(load_data, X_bbc_val_files)))\n",
    "\n",
    "X_techCrunch_gpt_train = np.array(list(map(load_data, X_techCrunch_train_files)))\n",
    "X_techCrunch_gpt_test = np.array(list(map(load_data, X_techCrunch_test_files)))\n",
    "X_techCrunch_gpt_val = np.array(list(map(load_data, X_techCrunch_val_files)))\n",
    "\n",
    "X_theVerge_gpt_train = np.array(list(map(load_data, X_theVerge_train_files)))\n",
    "X_theVerge_gpt_test = np.array(list(map(load_data, X_theVerge_test_files)))\n",
    "X_theVerge_gpt_val = np.array(list(map(load_data, X_theVerge_val_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('20_BBC.txt',\n",
       " 'Consider a doctor who has two patients who are grimacing and using similar words to describe their pain. Can the doctor be sure they are experiencing similar levels of pain? What if one habitually underestimates their suffering? What if one has been in pain for a long time and grown used to it? And what if the doctor has certain prejudices that mean they are more likely to believe one patient than the other? Pain is a difficult beast to grapple with, hard to measure and therefore to treat. Pain can be an important distress signal and failing to investigate it could mean a missed opportunity to save a life – or it may be something much more minor.\\xa0 For such a universal experience, pain remains much of a mystery – especially the task of determining how much pain someone is in. \"We understand it so poorly,\" says Emma Pierson, a computer scientist at Stanford University researching pain. \"In particular, the fact that human doctors are frequently left flummoxed by why a patient is in pain suggests that our current medical understanding of pain is quite bad.\" The gold standard for pain analysis currently relies on patients self-reporting how they feel, relying, in different places, on either a numerical scale (0 as no pain, 10 as worst pain), or a system of smiley faces. \"Step one in treating pain adequately is measuring it accurately and that\\'s the challenge,\" says Carl Saab, who leads a pain research team at Cleveland Clinic in Ohio. \"Nowadays the standard of care is based on \\'smiley faces\\' that riddle ER rooms.\" This system can be confusing for patients, he says, and especially problematic when treating children and non-communicative patients. Then there is the problem about whether the patient\\'s rating is believed. One study found a widespread notion that people tend to exaggerate the level of pain they are in, despite little evidence to suggest such exaggeration is common. You might also like to read:  Without an objective way to measure pain, there is room for bias to creep into clinicians\\' decisions. \"Pain has a particularly large impact on underserved populations, and their pain is particularly likely to be ignored,\" says Pierson. Unfortunately, false beliefs about pain are widely held among physicians. In 2016, one study found that 50% of white medical students and residents in the US held very dangerous and false ideas about black people and their experience of pain. Another study found that almost half of medical students heard negative comments about black patients by their senior colleagues, and those students\\' level of racial bias grew significantly in their first four years of medical training. Such biases date back to historical attempts to justify slavery, including false claims that black people had thicker skin and different nerve endings. Now, black patients in the US are 40% less likely to have their pain treated than white patients. Hispanic patients, meanwhile, are 25% less likely than white patients to have their pain treated. Racial discrimination is not the only form of prejudice that influences pain treatment. Biases around \"hysterical women\" are still well known in medicine, particularly around pain. A review of 77 separate research studies revealed that terms like \"sensitive\" and \"complaining\" are more often applied to women\\'s reports of pain. One study of 981 people found that women who came to emergency care due to pain were less likely to receive any pain relief at all, and they had to wait 33% longer than men to be treated. In addition, when men and women reported similar levels of pain, men were given stronger medication to treat it. Social expectations about what is \"normal behaviour\" for men and women are at the root of these patterns, says Anke Samulowitz, who researches gender bias at the University of Gothenburg in Sweden. These biases add up to \"medically unjustified differences in the way men and women are treated in health care\". There are, she notes, sometimes genuine reasons why men and women might receive different treatment for a particular complaint. \"Differences associated with hormones and genes should sometimes lead to differences in, for example, pain medication,\" she says. \"But all observed differences in the treatment of men and women with pain cannot be explained by biological differences.\" The accepted ways of estimating a patient\\'s pain leave room for physicians\\' bias to creep in (Credit: Getty Images) Could new technologies help provide a way to circumvent prejudice and bias about pain in medicine? Several innovations are being developed to try to plug this gap to provide an objective \"readout\" of the extent of someone\\'s pain. These technologies rely on finding \"biomarkers\" for pain – measurable biological variables that correlate with the experience of pain. \"Without biomarkers we will not be able to properly diagnose – and adequately treat – pain,\" says Saab. \"We will not be able to predict the likelihood of someone with acute back injury to transition to chronic treatment-resistant pain, and we will not be able to objectively monitor response to novel therapies in clinical trials.\" There are several candidates for biomarkers. Researchers in Indiana have developed a blood test to identify when a very specific set of genes involved in the body\\'s response to pain is activated. Levels of these biomarkers could indicate not only that someone is in pain, but how severe it is. Brain activity could be another useful biomarker. Saab and his team, while he was still at Brown University, devised an approach which measures the ebb and flow of a type of brain activity known as theta waves, which the team found were elevated during pain. Saab also found that administering analgesics reduced theta activity to normal levels. The team\\'s work has since been independently replicated by other labs. However, Saab sees theta-wave-based assessment of pain as an add-on rather than a replacement to current methods of measuring pain. \"We will never be able to know for sure how someone feels, be it pain or another mental state,\" says Saab. \"The verbal report of the patient should always remain as the \\'ground truth\\' for pain. I envision this being used as an adjunct diagnostic, especially in cases where the verbal report is unreliable: children, adults with altered mental status, non-communicative patients.\" Saab makes a distinction between acute pain, which functions as an alarm, \"in which case we should not be ignoring it,\" and chronic pain. Biomarkers could offer an objective way to measure a person\\'s pain, which could be less prone to human bias (Credit: Getty Images) Sometimes, a closer analysis of the injury or condition causing the pain could help to make treatments better and fairer. The Kellgren and Lawrence system, first proposed in 1957, looks at the severity of physical changes to the knee caused by osteoarthritis. One of the criticisms of it is that patients on low incomes, or from minority groups, often experience higher levels of pain from the condition. This deals a double blow to these individuals. \"Because these severity measures heavily influence who gets knee surgery, underserved groups may be under-referred for surgery,\" Pierson says. Pierson and her colleagues at Stanford developed a new algorithm which could address that. \"We use a deep learning approach to search for additional pain-relevant features in the knee X-ray the doctor might be missing, that might explain underserved patients\\' higher pain, by training a deep learning algorithm to predict pain from knee X-rays. \"So you could imagine, basically, using this algorithm to help better allocate surgery, by flagging to the doctor, \\'You said this patient doesn\\'t have physical knee damage, but here\\'s some indication in the X-ray that they might – do you want to take another look?\\'\" The algorithm still has some way to go to reach the real world, Pierson says, with challenges to overcome that are common across the field of AI in medicine: deployment, and training humans and algorithms to work well together. But she is excited that their algorithm finds signals within the knee that predict pain and could help to narrow the pain gap, saying this work highlights the potential of AI to reduce bias in healthcare. \"I am often drawn to problems where medical knowledge is clearly inadequate and this particularly harms populations medicine has historically ignored, such as racial minorities and women,\" Pierson says. She notes, however, that algorithms such as hers won\\'t solve the whole problem – for knee osteoarthritis. \"It\\'s not like our algorithm does some fantastically magical job of predicting pain,\" she says. \"But we\\'re comparing to a baseline understanding of pain which is quite bad, and to a severity score which was developed decades ago in heavily white British populations, and it\\'s just not that hard to improve on those baselines.\" The University of Gothenburg\\'s Samulowitz points out that relying on technology to reduce bias can introduce its own challenges too. For instance, there is the question of bias in the application of technology. \"Around one-fifth of the general population is affected by moderate to severe pain. Most of them seek medical treatment in primary care. Will all of them get a brain scan pain measurement or will the selection be biased? Research has shown that more men than women get referrals to somatic examinations, more women receive referrals to psychologists. There is a risk of gender bias in who will get an objective pain measurement.\" Despite these challenges ahead, Saab believes there is appetite for change in the field of pain. \"Clinicians are saying, \\'Look, we can\\'t base our clinical workflow on this, it\\'s not how medicine should be practiced.\\' When you have a high temperature, you use a thermometer. When you have high blood pressure, you test your blood concentrations. In this case, people come with pain, and we show them smiley faces.\" -- Join one million Future fans by liking us on Facebook, or follow us on Twitter or Instagram. If you liked this story, sign up for the weekly bbc.com features newsletter, called \"The Essential List\" – a handpicked selection of stories from BBC Future, Culture, Worklife, Travel and Reel delivered to your inbox every Friday. ',\n",
       " 'Pain is an inherent part of the human experience, but measuring it has always been a challenge. The task is made even harder by the subjective nature of pain - everyone experiences it differently. This can lead to doctors underestimating or exaggerating a patient\\'s pain, or dismissing it altogether. Biases around race and gender can also play a role, with some groups being underserved and their pain ignored. But a new wave of technologies is beginning to change this. These technologies provide objective measures of pain, using biomarkers like brain activity or blood tests. Algorithms are also being developed to better allocate treatment options based on the level of physical damage to the patient\\'s body. However, relying on technology to reduce bias has its own challenges. Research has suggested that biases can extend into the application of technology. For instance, if a particular biomarker has been shown to be more effective among certain groups, this could result in biases in treatment allocation. Nevertheless, there is a growing appetite for change in the field of pain management. Clinicians say that the current methods are inadequate and not how medicine should be practiced. So what are these new technologies, and how could they be used to counter bias in medicine?. Objective measures of pain. Traditionally, patients self-report their level of pain using subjective scales, such as the visual analogue scale (VAS) or numerical rating scale (NRS). These scales ask patients to rate their pain on a scale of 1 to 10, or to mark a spot on a line to indicate how severe their pain is. But these subjective measures have limitations. Patients can underestimate or exaggerate their pain, or be influenced by their psychological state. This can lead to doctors not taking their pain seriously, or to patients being prescribed unnecessary pain medication. Objective measures, on the other hand, can provide a more accurate assessment of pain. These measures use biomarkers - biological signals that indicate the presence or severity of pain. One biomarker that has been studied extensively is brain activity. Pain activates specific areas of the brain, such as the somatosensory cortex and the anterior cingulate cortex. Researchers can measure this activity using functional magnetic resonance imaging (fMRI) or electroencephalography (EEG). Studies have shown that brain activity is a reliable indicator of pain. For instance, a 2015 study found that fMRI could accurately predict whether a person was experiencing pain, and how severe it was, with 90% accuracy. Another biomarker being studied is blood tests. When tissues in the body are damaged, they release chemicals called cytokines. These cytokines can be detected in the bloodstream and are a reliable indicator of pain. Researchers are developing tests that can measure the concentration of these cytokines, providing an objective measure of pain. Algorithms for pain management. Objective measures of pain have the potential to revolutionize pain management by providing doctors with a more accurate assessment of their patients\\' pain. But how can this information be used to better allocate treatment options?. This is where algorithms come in. Algorithms are computer programs that can analyze vast quantities of data and make decisions based on that data. In the context of pain management, algorithms can help doctors to identify the most effective treatment options for their patients. One algorithm being developed is the \"pain probability score\" (PPS). This algorithm takes into account the patient\\'s personal factors, such as age, gender, and medical history, as well as objective measures of pain such as brain activity or cytokine levels. The algorithm then calculates the probability of the patient experiencing chronic pain, and recommends the most appropriate treatment option based on this probability. Another algorithm being developed is the \"physiological prioritization score\" (PPscore). This algorithm uses physiological data, such as heart rate variability and respiration rate, to identify patients who are at risk of developing chronic pain. The algorithm then recommends preventive measures, such as physiotherapy or cognitive-behavioral therapy, to reduce the risk of chronic pain. Challenges and limitations. While these new technologies offer exciting possibilities for pain management, they also come with challenges and limitations. One of the biggest challenges is ensuring that the technology is applied in a way that minimizes bias. For instance, if a particular biomarker has been shown to be more effective among certain groups, this could result in biases in treatment allocation. To minimize this risk, researchers need to ensure that their studies are diverse and inclusive, so that the data is representative of all groups. Another challenge is developing algorithms that are robust and accurate. Algorithms rely on large datasets to be trained, and if the data is biased or incomplete, the algorithm may make inaccurate recommendations. Researchers need to ensure that their datasets are diverse and representative, and that the algorithms are continually updated as new data becomes available. There are also limitations to the technology itself. Objective measures of pain, such as brain activity or cytokine levels, are not perfect indicators of pain. They can be influenced by other factors, such as the patient\\'s psychological state or their medication use. Algorithms are also not foolproof, and can make mistakes or overlook important factors. Conclusion. Pain is a difficult and complex phenomenon to measure. But new technologies, such as objective measures of pain and algorithms for pain management, offer the potential to revolutionize pain management by providing doctors with a more accurate assessment of their patients\\' pain. However, these technologies come with challenges and limitations. Bias can still play a role in the application of technology, and algorithms need to be developed carefully to ensure that they are robust and accurate. While these challenges are significant, they are not insurmountable, and researchers are actively working to address them. As clinicians continue to push for more accurate and effective methods for managing pain, it is clear that the future of pain management lies in a combination of technology and human expertise. By working together, doctors and technology can provide better care for patients and reduce the burden of pain on society as a whole.')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_bbc_train_files[1], X_bbc_train[1], X_bbc_gpt_train[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT train: 435, 435\n",
      "GPT test: 95, 95\n",
      "GPT val: 93, 93\n"
     ]
    }
   ],
   "source": [
    "X_gpt_train = np.hstack((X_bbc_gpt_train, X_techCrunch_gpt_train, X_theVerge_gpt_train))\n",
    "Y_gpt_train = np.ones(len(X_gpt_train))\n",
    "print(f'GPT train: {len(X_gpt_train)}, {len(Y_gpt_train)}')\n",
    "\n",
    "X_gpt_test = np.hstack((X_bbc_gpt_test, X_techCrunch_gpt_test, X_theVerge_gpt_test))\n",
    "Y_gpt_test = np.ones(len(X_gpt_test))\n",
    "print(f'GPT test: {len(X_gpt_test)}, {len(Y_gpt_test)}')\n",
    "\n",
    "X_gpt_val = np.hstack((X_bbc_gpt_val, X_techCrunch_gpt_val, X_theVerge_gpt_val))\n",
    "Y_gpt_val = np.ones(len(X_gpt_val))\n",
    "print(f'GPT val: {len(X_gpt_val)}, {len(Y_gpt_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Title: The Three Russian Cyber-Attacks the West Most Fears. As tensions rise between Russia and the West, US President Joe Biden and UK cyber-authorities have called for increased cybersecurity measures amid intelligence suggesting that Russia is planning a cyber-attack on the United States. Although Russia has denied these accusations, experts fear that the country's cyber-superpower status and arsenal of cyber-tools could pose serious threats to Ukraine's allies. Three specific cyber-attacks have raised concerns among cybersecurity experts. The first incident that experts find particularly worrisome is the BlackEnergy attack in 2015. This hack disrupted Ukraine's electricity grid and resulted in a short-term blackout for approximately 80,000 customers. The attack was attributed to Russian military hackers, highlighting their ability to infiltrate crucial infrastructure systems. The fear is that Russia could attempt similar attacks against Western countries to showcase its capabilities and instill damage. However, experts note that successfully executing such attacks on complex engineering systems reliably and causing prolonged effects is extremely challenging. Another significant cyber-attack that heightens concerns is the Industroyer attack. In 2016, this attack affected power in Kyiv, the capital of Ukraine, for approximately an hour. Like the BlackEnergy attack, Russian military hackers were held responsible. These incidents demonstrate Russia's capacity to disrupt power grids, posing a significant threat to nations reliant on electricity-dependent infrastructure. While the immediate impact may be limited, the potential for long-term consequences cannot be ignored. The most damaging and costly cyber-attack to date, the NotPetya attack, continues to worry cybersecurity experts. This attack, blamed on a group of Russian military hackers, caused an estimated $10 billion in damages, spreading globally and affecting thousands of companies. The attack targeted critical infrastructure and disrupted operations across various sectors. Such attacks further highlight the potential for mass chaos, economic instability, and even loss of life resulting from cyber-offensives. To maximize disruption and maintain plausible deniability, experts suggest that Russia may instruct cyber-crime groups to coordinate attacks on US targets. The recent cyber-criminal attacks on the Colonial Pipeline and JBS serve as prime examples of the chaos that can be caused. These attacks not only impacted the economy but also revealed vulnerabilities within critical infrastructure systems. Russian coordination with cyber-criminal groups offers a convenient way for the state to deflect blame while achieving their goals. In the event of a significant cyber-attack causing loss of life or substantial damage, the United States would likely respond. However, invoking Article 5, the collective defense clause within NATO, is considered unlikely due to concerns about further escalation. Any response would need to be carefully considered to avoid exacerbating an already tense situation between Russia and the West. In response to the rising threat, President Biden has urged private companies and organizations in the United States to enhance their cybersecurity measures. The UK's cyber-authorities have echoed these calls for increased precautions. It is vital for both the public and private sectors to remain vigilant and actively invest in cybersecurity to protect critical infrastructure and sensitive information from potential state-sponsored cyber-attacks. It is worth noting that while these concerns focus on Russia, numerous other state-sponsored cyber-attacks from countries like China, North Korea, and Iran also pose significant threats to global cybersecurity. As nations increase their reliance on interconnected systems, the potential for cyber-warfare to disrupt societies continues to grow. As the battle for cybersecurity continues, it is clear that the West must remain proactive and diligent in safeguarding its critical infrastructure. Cooperation between governments, private companies, and international organizations is essential to not only detect and respond to cyber-attacks but also to develop and enforce stronger cybersecurity measures. Only by staying one step ahead can the West deter, defend, and mitigate the evolving threats posed by state-sponsored cyber-attacks. \""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_gpt_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data= [X_gpt_train, X_gpt_test, X_gpt_val]\n",
    "\n",
    "filename = 'data_pickles/train_test_val_GPT'\n",
    "file = open(filename, 'wb')\n",
    "pickle.dump(gpt_data, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT PosTags Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    content = None\n",
    "    pos_tags = None\n",
    "    folder = 'BBC/GPT_BBC_news/' if 'BBC' in file else 'TechCrunch/GPT_TechCrunch_news/' if 'TechCrunch' in file else 'TheVerge/GPT_TheVerge_news/'\n",
    "    with open(f'{folder}{file}', 'r', encoding=\"utf-8\") as news:\n",
    "        content = news.read()\n",
    "\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        pos_tag_tuples = nltk.pos_tag(tokens, lang='eng')\n",
    "        pos_tags = [tag[1] for tag in pos_tag_tuples]\n",
    "        \n",
    "    return pos_tags\n",
    "\n",
    "# print(load_data(X_bbc_GPT_train_files[1]))\n",
    "\n",
    "X_bbc_GPT_posTags_train = np.array(list(map(load_data, X_bbc_GPT_train_files)))\n",
    "X_bbc_GPT_posTags_test = np.array(list(map(load_data, X_bbc_GPT_test_files)))\n",
    "X_bbc_GPT_posTags_val = np.array(list(map(load_data, X_bbc_GPT_val_files)))\n",
    "\n",
    "X_techCrunch_GPT_posTags_train = np.array(list(map(load_data, X_techCrunch_GPT_train_files)))\n",
    "X_techCrunch_GPT_posTags_test = np.array(list(map(load_data, X_techCrunch_GPT_test_files)))\n",
    "X_techCrunch_GPT_posTags_val = np.array(list(map(load_data, X_techCrunch_GPT_val_files)))\n",
    "\n",
    "X_theVerge_GPT_posTags_train = np.array(list(map(load_data, X_theVerge_GPT_train_files)))\n",
    "X_theVerge_GPT_posTags_test = np.array(list(map(load_data, X_theVerge_GPT_test_files)))\n",
    "X_theVerge_GPT_posTags_val = np.array(list(map(load_data, X_theVerge_GPT_val_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bbc_GPT_train_files[0], X_bbc_GPT_posTags_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT train posTags: 435\n",
      "GPT test posTags: 95\n",
      "GPT val posTags: 93\n"
     ]
    }
   ],
   "source": [
    "X_gpt_train_posTags = np.hstack((X_bbc_GPT_posTags_train, X_techCrunch_GPT_posTags_train, X_theVerge_GPT_posTags_train))\n",
    "print(f'GPT train posTags: {len(X_gpt_train_posTags)}')\n",
    "\n",
    "X_gpt_test_posTags = np.hstack((X_bbc_GPT_posTags_test, X_techCrunch_GPT_posTags_test, X_theVerge_GPT_posTags_test))\n",
    "print(f'GPT test posTags: {len(X_gpt_test_posTags)}')\n",
    "\n",
    "X_gpt_val_posTags = np.hstack((X_bbc_GPT_posTags_val, X_techCrunch_GPT_posTags_val, X_theVerge_GPT_posTags_val))\n",
    "print(f'GPT val posTags: {len(X_gpt_val_posTags)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_data_posTags= [X_gpt_train_posTags, X_gpt_test_posTags, X_gpt_val_posTags]\n",
    "\n",
    "filename = 'data_pickles/train_test_val_GPT_posTags'\n",
    "file = open(filename, 'wb')\n",
    "pickle.dump(gpt_data_posTags, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Janelamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de tokens iguais em todas as janeals (menos a ultima, possivelmente pode ficar com menos).\n",
    "# Numero de frases pode variar pois se a quantidade de tokens der no meio de uma frase, a frase inteira ira para a janela.\n",
    "# Evitar problemas de cortar palavras ao meio e/ou fixar um numero de frases (uma frase pode ser gigante e aoutra minuscula)\n",
    "\n",
    "def sample_division(sample, dvision_qtd=2):\n",
    "    n_words = len(nltk.word_tokenize(sample))\n",
    "    sentences = nltk.sent_tokenize(sample)\n",
    "    n_tokens_sentences = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for token in nltk.word_tokenize(sentence):\n",
    "            n_tokens_sentences += 1\n",
    "\n",
    "    tokens_per_sample = n_words // dvision_qtd\n",
    "    # print(tokens_per_sample)\n",
    "    new_samples = []\n",
    "    current_sample = ''\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(nltk.word_tokenize(current_sample)) + len(nltk.word_tokenize(sentence)) < tokens_per_sample:\n",
    "            if index == 0 or current_sample == '':\n",
    "                current_sample += sentence\n",
    "            elif current_sample != '' and index != 0:\n",
    "                current_sample += ' '\n",
    "                current_sample += sentence\n",
    "            \n",
    "\n",
    "        else:\n",
    "            current_sample += sentence\n",
    "            if current_sample != '':\n",
    "                new_samples.append(current_sample)\n",
    "                current_sample = ''\n",
    "\n",
    "        if index >= len(sentences) - 1 and current_sample != '':\n",
    "            new_samples.append(current_sample)\n",
    "            current_sample = ''\n",
    "\n",
    "    # print(len(new_samples))\n",
    "    # for sample in new_samples:\n",
    "    #     print(len(nltk.word_tokenize(sample)))\n",
    "    # print(new_samples)\n",
    "\n",
    "    return new_samples\n",
    "\n",
    "\n",
    "\n",
    "# sample_division(X_gpt_train[0])\n",
    "# sample_division(X_human_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "divide_in = 2\n",
    "\n",
    "# Human -------------------------\n",
    "\n",
    "extended_X_human_train = []\n",
    "for data in X_human_train:\n",
    "    extended_X_human_train.extend(sample_division(data, divide_in))\n",
    "extended_Y_human_train = np.zeros(len(extended_X_human_train))\n",
    "extended_X_human_train = np.array(extended_X_human_train)\n",
    "\n",
    "extended_X_human_test = []\n",
    "for data in X_human_test:\n",
    "    extended_X_human_test.extend(sample_division(data, divide_in))\n",
    "extended_Y_human_test = np.zeros(len(extended_X_human_test))\n",
    "extended_X_human_test = np.array(extended_X_human_test)\n",
    "\n",
    "extended_X_human_val = []\n",
    "for data in X_human_val:\n",
    "    extended_X_human_val.extend(sample_division(data, divide_in))\n",
    "extended_Y_human_val = np.zeros(len(extended_X_human_val))\n",
    "extended_X_human_val = np.array(extended_X_human_val)\n",
    "\n",
    "\n",
    "# GPT -------------------------\n",
    "\n",
    "extended_X_gpt_train = []\n",
    "for data in X_gpt_train:\n",
    "    extended_X_gpt_train.extend(sample_division(data, divide_in))\n",
    "extended_Y_gpt_train = np.ones(len(extended_X_gpt_train))\n",
    "extended_X_gpt_train = np.array(extended_X_gpt_train)\n",
    "\n",
    "extended_X_gpt_test = []\n",
    "for data in X_gpt_test:\n",
    "    extended_X_gpt_test.extend(sample_division(data, divide_in))\n",
    "extended_Y_gpt_test = np.ones(len(extended_X_gpt_test))\n",
    "extended_X_gpt_test = np.array(extended_X_gpt_test)\n",
    "\n",
    "extended_X_gpt_val = []\n",
    "for data in X_gpt_val:\n",
    "    extended_X_gpt_val.extend(sample_division(data, divide_in))\n",
    "extended_Y_gpt_val = np.ones(len(extended_X_gpt_val))\n",
    "extended_X_gpt_val = np.array(extended_X_gpt_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((712,), (240,), (233,), (714,), (240,), (237,))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_X_human_train.shape, extended_X_human_test.shape, extended_X_human_val.shape, extended_X_gpt_train.shape, extended_X_gpt_test.shape, extended_X_gpt_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sasha, Katie and Ruby have been friends since primary school. They moved up to their secondary together and spend much of their free time in each other\\'s company. Like other teenagers, messaging each other on social media comes naturally to them. But does this add to - or detract from - their friendship? New research has thrown up some welcome news for children and parents alike, many of whom worry about the amount of screen time young people have. Dr Rebecca Anthony, a research associate at Cardiff University\\'s public health body Decipher  said the findings arose while studying responses to the 2019 Student Health and Wellbeing Survey. \"What they were finding is there is a small but significant association between time spent online and depressive symptoms, but a review came out and said we\\'re not really looking at the nuances of social media,\" she said. \"We used the survey to look at who young people were engaging with as well as how long they were spending online.\" And, for once, there was good news about screen time. Better wellbeing was found to be associated with those who spent their time online speaking to their close and wider group of friends who they already knew offline. Dr Anthony said it was a \"very clear message\" that spending time speaking to people online the teenagers did not know was associated with \"much poorer wellbeing\" and was stronger for teenage girls who only messaged people online. The research may provide some welcome news after another survey found teenagers in Wales had exercised less and spent more time in front of a screen during the pandemic, with almost a quarter experiencing adverse mental health symptoms. But how does Dr Anthony\\'s research translate to the three friends\\' experiences? The trio, now all 14 or 15, started using mobile phones in the last year of primary school, and at first just used WhatsApp to communicate with their closer friends.This widened out after they moved to secondary school, when larger numbers of people would form messaging groups.',\n",
       " 'Sasha said: \"In terms of WhatsApp, I honestly think I used it more [back then] because everyone had just gotten a phone and all the group chats were blowing up all the time.\" The girls said they generally found this app a positive thing as they found their feet in secondary school. As they got older, different apps started to take over. Ruby said: \"The thing that people got addicted to on their phones was TikTok, but the way people talked to each other was Snapchat and Instagram. \"There\\'s more drama and things on Snapchat and Instagram. People get into arguments, and that\\'s more impactful on our lives.\" The girls agreed Snapchat was the main way larger groups communicate but, for their own more intimate friend group, they revert to WhatsApp. \\'It\\'s always there\\' Ruby said that while messaging her close friends from home in the evenings was generally a positive experience, being in other group chats had sometimes left her with \"a negative feeling, even if nothing awful has happened\". Do they ultimately think technology has improved both their friendship and their happiness? Katie said: \"I think it does, because if you\\'re feeling really sad or bored or something, it\\'s always there. \"If you\\'re feeling really happy about something you can tell them now and it enhances how you\\'re feeling, and brings you more enjoyment about it.\" Sasha described it as a \"comfort\" that a person can always be contacted, even if it is to sort out an argument. \"You can text and say \\'sorry, I didn\\'t mean to say that\\', whereas if you didn\\'t have that you would just have to sit there and think about it,\" she said. Evie Kwan, 17, is a Welsh Youth Parliament representative from Cowbridge, Vale of Glamorgan, with an interest in mental health.She said the ability to check in with people, \"ask how they are doing and if they need a chat or something face-to-face\" was valuable.',\n",
       " '\"However, when it comes to things like loneliness, having that constant stream of knowing what everyone\\'s doing, having them message you constantly, it\\'s like a whole other world.\" She added: \"It can make you feel isolated and lonely if you\\'re not participating in these chats in social media apps.\" For Dr Linda Papadopoulos, psychologist and ambassador for online safety body Internet Matters, the most vital thing parents can do is get to know what their children are doing. \"Look out for how your kids are interacting online and how does it make them feel in the same way that you would with anything - they start a new club, they start a new school,\" she said. \"Did your kid go from being open and happy to becoming very, very withdrawn? Did your kid go from being very quiet to being very, very agitated when you take their tech off them?\" She uses the analogy of the five-a-day fruit and vegetables health message. \"It\\'s wonderful that I connect with my friends online. But if that\\'s all I\\'m doing, that\\'s a problem. \"That\\'s one vegetable. The other vegetable is, have you had a play date? Have you gone out? Have you moved? Have you interacted - just by text or have you spoken?\" Dr Anthony said she had been surprised at how \"switched on\" some teenagers were with self-control, including turning off notifications when they were doing homework. The girls have seemingly worked this out for themselves when it comes to talking online. \"As long as you\\'re able to control your limits, it\\'s positive,\" said Katie. Ruby added: \"There are going to be people that have addictions to social media and then it will be a negative, but for people that can use it in moderation it\\'s always going to be a positive.\"',\n",
       " 'When Russia initiated its full-scale invasion of Ukraine, a second, less visible battle in cyberspace got under way. The BBC\\'s cyber correspondent Joe Tidy travelled to Ukraine to speak to those fighting the cyber war, and found the conflict has blurred the lines between those working for the military and the unofficial activist hackers. When I went to visit Oleksandr in his one-bedroom flat in central Ukraine, I found a typically spartan set-up common to many hackers. No furniture or home comforts - not even a TV - just a powerful computer in one corner of his bedroom and a powerful music system in the other. From here, Oleksandr has helped temporarily disable hundreds of Russian websites, disrupted services at dozens of banks and defaced websites with pro-Ukraine messages. He is one of the most prominent hackers in the vigilante group, the IT Army of Ukraine - a volunteer hacking network with a Telegram group nearly 200,000-strong. For more than a year, he has devoted himself to causing as much chaos in Russia as possible. Even during our visit he was running complex software attempting to take his latest target - a Russian banking website - offline. Ironically though, he admits the idea for his favourite hack actually began with a tip from an anonymous Russian, who told them about an organisation called Chestny Znak - Russia\\'s only product authentication system. He was told all goods produced in Russia - including fresh food - have to be scanned for a unique number and a barcode supplied by the company from the moment of their creation at a factory, up till the moment of being sold. Oleksandr smiles as he describes how he and his team found a way to take the service offline, using a hacking tool that floods a computer system with internet traffic - known as a targeted DDoS (Distributed Denial-of-Service) attack. \"The economic losses were pretty high, I think. It was mind-blowing,\" says Oleksandr. In reality, it\\'s hard to gauge the disruption prompted by the hack, but for four days last April Chestny Znak posted regular updates about the DDoS attack on its official Telegram feed. Traders were offered advice and a helpline to call for assistance. Eventually Russia\\'s Ministry of Industry and Trade was forced to relax some rules on food labelling to allow perishables to be traded. More recently, around the first anniversary of the invasion, Oleksandr joined a team of hackers, called One Fist, to hijack Russian radio stations and broadcast the sound of fake air raid sirens and an alert message telling citizens to take shelter. \"We feel ourselves like military,\" says Oleksandr. \"When my country calls me to pick up a rifle I am ready, but hacking Russia now, I feel that I am helpful.\" Many experts predicted that people like Oleksandr - hacktivists - might play a part in the Ukraine conflict, but the scale of the activity has come as a shock - with hacker armies emerging on both sides. Officially Ukraine is defending itself from cyber attacks, aided by Western cyber military teams and private cyber-security companies - funded by millions of dollars in donations. But unofficial links are also beginning to emerge between vigilante groups going on the offensive and carrying out criminal attacks and military officials. On both sides, lines between targeted, state-sanctioned cyber-attacks and ad-hoc vigilante hacking have been blurred. The consequences could be far-reaching. On a visit to Ukraine\\'s cyber defence HQ in Kyiv, officials claim they have evidence that the Russian hacktivist gang, Killnet, which has a Telegram group of nearly 100,000, is working directly with the cyber section of the Russian military.\"These groups, like Killnet or the Cyber Army of Russia, started off carrying out DDoS attacks - but have since recruited more talented and skilled people,\" says Viktor Zhora, deputy chairman of the State Service of Special Communications.',\n",
       " 'He alleges the groups have consultants from the Russian military and are now capable of launching sophisticated cyber attacks. Russian commanders are uniting all of the hacktivist groups and activities into a single source of aggression in cyber-space against Ukraine and its allies, he claims. The link would be problematic for Russia if proven. Since the invasion, Russia has been relatively targeted in its cyber attacks - sticking to Ukrainian targets that are loosely linked to the war effort. But Killnet has called for and carried out disruptive - albeit temporary - attacks on hospital websites in both Ukraine and allied countries. The Geneva Convention, rules aimed to limit the savagery of physical wars, prohibit such attacks on civilians. But there\\'s no Geneva Convention for cyber warfare. The International Committee of the Red Cross argues that existing codes should apply - so targeting hospitals, for example, would constitute a breach. The fact that hacker attacks are being carried out on Nato countries could also trigger a collective response if they caused serious harm. The Russian government did not respond to requests for comment from the BBC - so instead, we went directly to Killnet\\'s leader, who goes by the moniker Killmilk. Killmilk refused to take part in a face-to-face interview, but after weeks of messages on Telegram he sent video responses to our questions - before breaking off communication. In his recordings, Killmilk boasts that the hackers devote 12 hours a day to Killnet. \"I see no equal in the world to Russian hackers. Useless and stupid Ukrainian hackers cannot succeed in confronting us,\" he says. Killmilk insisted his group is completely independent of Russian special services, asserting that he has a regular job as a factory loader and is \"a simple person\". Before the war and Killnet, he says he started a criminal DDoS-for-hire business - but, when the invasion began, he was determined to dedicate his hacking efforts to disrupting Ukraine and its allies. \"Wherever I am, my laptop and everything I need is always with me. In this, I am mobile and efficient and devote almost all my time to our movement.\" Whilst some hactivist vigilante groups, like the infamous Anonymous collective, have slowed down activities against Russia and moved on in the past three months, Killnet has increased its activity and is also focussing on the West - urged on by bombastic videos from Killmilk which show him urinating on flags representing Nato and the US. Over the Easter weekend, Killnet\\'s Telegram channel was used to create a pop-up team called KillNATO Psychos. Within hours, it was hundreds of members-strong and proceeded to carry out a wave of attacks which temporarily disrupted Nato member websites. The group also published a list of Nato staff email addresses and encouraged people to harass them. The allegation that Russian cyber military is working with criminal hacktivists will not come as a shock for many in the cyber-security world, which for years has accused Russia of harbouring some of the most prolific and profitable cyber-crime groups. However our visit to Ukraine confirmed that lines are blurring there too. A year ago, as the capital Kyiv braced for attack, Roman was helping carry out criminal hacks and building software for the war effort - as part of a volunteer group he co-founded called IT Stand for Ukraine. But now, he has officially been recruited by his country\\'s cyber military. We met in a park near his training headquarters in the city of Zhytomyr, two hours west of Kyiv. As a vigilante-turned-military man, Roman has a unique perspective.He chose not to go into details about his current role, but says part of his job is finding ways to comb through piles of data and leaked information from the cyber war.',\n",
       " 'Even before he was recruited, Roman confirms his hacking team worked directly with Ukrainian authorities. \"We started to communicate with state forces doing the same as us and we began kind of synchronising our operations. They basically started to give us some targets and say what to do, when to do,\" he says. Roman says one of the most impactful attacks was when his team disabled ticket machines for a southern Russian railway network. When challenged about the disruption this would have caused everyday people in Russia, Roman shrugs. It\\'s the kind of attack that Ukraine\\'s cyber military could never publicly carry out themselves. Since the start of the full-scale invasion, Ukraine has been at pains to present itself as the defender, not the attacker. Could Ukrainian cyber commanders be using vigilantes to stretch the front line of the conflict into the Russian state? Mykhailo Fedorov, Ukraine\\'s deputy prime minister and minister for digital transformation, is responsible for the department which controversially set up the Telegram group for Oleksandr\\'s IT Army of Ukraine. Since then, the government claims to have had no involvement in the hacktivist network. Mr Fedorov denies accusations he is encouraging offensive attacks against Russian civilian targets. But he said he is confident that Ukraine has the \"moral right to do everything it can to protect the lives of our citizens\". \"I think that in a year of the full-scale war, Ukrainian hackers have shown that, despite such an invasion, they operate ethically enough - and do not cause excessive damage to any subjects besides the ones of the Russian Federation that are involved in the war,\" he says. But Ukrainian hacktivists are not just hacking the Russian war machine. Despite the Ukrainian army not condoning the targetting of institutions like healthcare, hacks are being organised to cause as much disruption as possible to the Russian people. One of the co-ordinators of the IT Army of Ukraine, Ted, proudly showed us his collection of angry comments from ordinary Russian customers who have experienced disruption as a result of their activities. His website includes a hacker leaderboard - I ask whether his group could be \"gamifying\" criminal hacking - and whether this created a danger of escalation. \"Talking from the standpoint of Western laws, I think, yes, there is a danger of gamification of illegal hacking. But what we need to realise is that when war is coming to your country, there are no good ways or bad ways to fight.\" Some predict the severity of attacks will increase from Russia as it struggles on the physical battlefield. Ukrainian officials say the worst attacks are not coming from hacktivists, but from the Russian military, which is co-ordinating cyber attacks with physical attacks on targets such as energy grids. Back at Ukraine\\'s cyber defence HQ, Mr Zhora says Russia has so far failed to deliver the kind of cyber attacks that experts feared - but that\\'s not due to lack of effort - and their attacks would be more disruptive if Ukraine was weaker. Some sources have also told me that Ukraine has been worse affected by Russian hackers than its commanders are admitting. As with all aspects of war, the fog of war is hard to penetrate and that\\'s especially true in cyber-space. Additional reporting by producer Peter Ball')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended_X_human_train[0], extended_X_human_train[1], extended_X_human_train[2], extended_X_human_train[3],extended_X_human_train[4], extended_X_human_train[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_posTags(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens, lang='eng')\n",
    "\n",
    "    return np.array([tag[1] for tag in pos_tags])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Text Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870, 870, 870, 190, 190, 190, 186, 186, 186)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sem janelamento\n",
    "train_data = np.hstack((X_human_train, X_gpt_train))\n",
    "train_labels = np.hstack((Y_human_train, Y_gpt_train))\n",
    "\n",
    "test_data = np.hstack((X_human_test, X_gpt_test))\n",
    "test_labels = np.hstack((Y_human_test, Y_gpt_test))\n",
    "\n",
    "val_data = np.hstack((X_human_val, X_gpt_val))\n",
    "val_labels = np.hstack((Y_human_val, Y_gpt_val))\n",
    "\n",
    "train_data_posTag = np.hstack((X_human_train_posTags, X_gpt_train_posTags))\n",
    "test_data_posTag = np.hstack((X_human_test_posTags, X_gpt_test_posTags))\n",
    "val_data_posTag = np.hstack((X_human_val_posTags, X_gpt_val_posTags))\n",
    "\n",
    "\n",
    "# # Com janelamento\n",
    "# train_data = np.hstack((extended_X_human_train, extended_X_gpt_train))\n",
    "# train_labels = np.hstack((extended_Y_human_train, extended_Y_gpt_train))\n",
    "\n",
    "# test_data = np.hstack((extended_X_human_test, extended_X_gpt_test))\n",
    "# test_labels = np.hstack((extended_Y_human_test, extended_Y_gpt_test))\n",
    "\n",
    "# val_data = np.hstack((extended_X_human_val, extended_X_gpt_val))\n",
    "# val_labels = np.hstack((extended_Y_human_val, extended_Y_gpt_val))\n",
    "\n",
    "# train_data_posTag = [extract_posTags(text) for text in train_data]\n",
    "# test_data_posTag = [extract_posTags(text) for text in test_data]\n",
    "# val_data_posTag = [extract_posTags(text) for text in val_data]\n",
    "\n",
    "len(train_data), len(train_labels), len(train_data_posTag), len(test_data), len(test_labels), len(test_data_posTag), len(val_data), len(val_labels), len(val_data_posTag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hundreds of Russian and Chinese state propaganda accounts are thriving on Twitter after Elon Musk wiped out the team that fought these networks, the BBC has found. The unit worked to combat \"information operations\", coordinated campaigns by countries such as Russia, China, and Iran, made to influence public opinion and disrupt democracy. But experts and former employees say the majority of these specialists resigned or were laid off, leaving the platform vulnerable to foreign manipulation. The BBC has spoken to several of them. They asked for anonymity, citing non-disclosure agreements and threats they received online. \"The whole human layer has been wiped out. All Twitter has left are automated detections systems,\" a former senior employee said. In a BBC interview on Tuesday, Musk claimed there was \"less misinformation [on Twitter] rather than more\" under his tenure. He did not comment on active state troll farms on the platform nor the team that used to fight them.  We approached Twitter for comment but received no response other than a poo emoji - the standard auto-reply from the company to any press enquiry. \\'Troll farms\\' Organised groups of people posting coordinated messages are called \\'troll farms.\\' The term was first used by Russian reporters who exposed one of roughly 300 paid employees run by Yevgeny Prigozhin, head of the Wagner mercenary group. Since then, troll farms influencing elections and public opinion have been uncovered in many countries, from Poland and Turkey to Brazil and Mexico. They have also been used as a propaganda tool in ethnic conflicts and wars. Now, a new group of Russian trolls is active on Twitter.  It supports Putin\\'s war in Ukraine, ridicules Kyiv and the West, and attacks independent Russian-language publications, including the BBC Russian Service. Many of these trolls\\' accounts have been suspended, but dozens are still active.  Darren Linvill, associate professor at the Clemson University Media Forensics Hub in South Carolina, says the network appears to originate from Prigozhin\\'s troll factory.  Mr Linvill and his colleagues have also discovered two similar Russian-language troll networks, but from an opposite camp. One tweets in support of Ukraine, and another promotes Russian opposition, including the jailed Putin critic Alexey Navalny.  While they have all the markings of troll accounts, including random numbers in the Twitter handles and coordinated behaviour, these networks appear to remain undetected by the platform. The Clemson University team is also tracking pro-Chinese accounts targeting users in both Chinese and English about issues of importance to the Chinese government.  With only a skeleton crew remaining, Twitter does not have resources to swiftly detect, attribute and take down this foreign propaganda, according to former employees. While the platform also established partnerships with research institutions that detected information operations, scholars say they have not heard anything from Twitter since November. Punching above its weight Experts have long warned about the dangers of foreign influence on social media. In 2018, the FBI said that fake accounts impersonating real Americans had played a central role in the Russian effort to meddle in the 2016 election. That was when Twitter and Facebook started hiring \"information operations\" specialists. \"I still remember the rage I felt when I saw accounts with names like \"Pamela Moore\" and \"Crystal Johnson\" purporting to be real Americans from Wisconsin and New York, but with phone numbers tracing back to St Petersburg, Russia,\" recalls Yoel Roth, Twitter\\'s former Trust and Safety head. Twitter has a fraction of Facebook\\'s reach and budget. But over the years, it built a small but capable team. While it could not match the resources of its rival social network, Twitter \"nonetheless punched above its weight\", says Lee Foster, an independent expert in information operations. Twitter hired people with backgrounds in cybersecurity, journalism, government agencies and NGOs who spoke an array of languages including Russian, Farsi, Mandarin, Cantonese, Spanish and Portuguese. One former investigator says: \"We needed people who would be able to understand: if Russia is likely to be the responsible actor behind this, what is its motivation to do this particular operation?\" He says he resigned because his team did not fit into \\'Twitter 2.0\\' that Musk was building. \"Our role was to help make the use of Twitter as safe as possible. And it did not feel like that was likely to continue as a priority.\" Countering propaganda worldwide The team worked in close contact, but separately from the ones countering misinformation. That is because state-run campaigns can use both fake news and factual stories to promote their messages. In 2016, Russian trolls targeted black voters in the US using real footage showing police violence. And in 2022, a coordinated network promoted negative - but sometimes accurate - news about the French contingent and United Nations missions in Africa\\'s Sahel region.  Both networks were taken down by Twitter. As similar information operations were conducted on different platforms, Twitter employees met with their peers at Meta and other companies to exchange information.  But at such meetings, Twitter\\'s investigators would be reminded of how small their operation was. \"Their team would be ten times the size of ours,\" says an investigator. Now even those resources are lacking. Without the team dedicated to fight coordinated campaigns, Twitter \"will slowly become more and more unsafe,\" says Linvill of Clemson University. With additional reporting from Hannah Gelbart. '"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization --. y = (x – min) / (max – min)\n",
    "\n",
    "def extract_posTag_features(text_posTag):\n",
    "    adjective_tags = ['JJ', 'JJ$', 'JJ+JJ', 'JJR', 'JJR+CS', 'JJS', 'JJT', ]\n",
    "    noun_tags = ['NN', 'NN$', 'NN+BEZ', 'NN+HVD', 'NN+HVZ', 'NN+IN', 'NN+MD', 'NN+NN', 'NNS', 'NNS$', 'NNS+MD', 'NP', 'NP$', 'NP+BEZ', 'NP+HVZ', 'NP+MD', 'NPS', 'NPS$', 'NR', 'NR$', 'NR+MD', 'NRS']\n",
    "    pronoun_tags = ['PN', 'PN$', 'PN+BEZ', 'PN+HVD', 'PN+HVZ', 'PN+MD', 'PP$$', 'PPL', 'PPLS', 'PPO', 'PPS', 'PPS+BEZ', 'PPS+HVD', 'PPS+HVZ', 'PPS+MD', 'PPSS', 'PPSS+BEM', 'PPSS+BER', 'PPSS+BEZ', 'PPSS+BEZ*', 'PPSS+HV', 'PPSS+HVD', 'PPSS+MD', 'PPSS+VB']\n",
    "    article_tags = ['AT']\n",
    "    conjunction_tags = ['CC', 'CS']\n",
    "    numeral_tags = ['CD', 'CD$', 'OD']\n",
    "    preposition_tags = ['IN', 'IN+IN', 'IN+PPO']\n",
    "    qualifier_tags = ['QL', 'QLP']\n",
    "    adverb_tags = ['RB', 'RB$', 'RB+BEZ', 'RB+CS', 'RBR', 'RBR+CS', 'RBT', 'RN', 'RP', 'RP+IN']\n",
    "    foreign_word = 'FW'\n",
    "    w_classification_tags = ['WDT', 'WDT+BER', 'WDT+BER+PP', 'WDT+BEZ', 'WDT+DO+PPS', 'WDT+DOD', 'WDT+HVZ', 'WP$', 'WPO', 'WPS', 'WPS+BEZ', 'WPS+HVD', 'WPS+HVZ', 'WPS+MD', 'WQL', 'WRB','WRB+BER', 'WRB+BEZ', 'WRB+DO', 'WRB+DOD', 'WRB+DOD*', 'WRB+DOZ', 'WRB+IN', 'WRB+MD']\n",
    "    modal_tags = ['MD', 'MD*', 'MD+HV', 'MD+PPSS', 'MD+TO']\n",
    "    a_determiner_tags = ['ABL', 'ABN', 'ABX', 'AP', 'AP$', 'AP+AP']\n",
    "    determiner_tags = ['DT', 'DT$', 'DT+BEZ', 'DT+MD', 'DTI', 'DTS', 'DTX']\n",
    "    verb_tags = ['BE', 'BED', 'BED*', 'BEDZ', 'BEDZ*', 'BEG', 'BEM', 'BEM*', 'BEN', 'BER', 'BER*', 'BEZ', 'BEZ*', 'DO', 'DO*', 'DO+PPSS', 'DOD', 'DOD*', 'DOZ', 'DOZ*', 'HV','HV*', 'HV+TO', 'HVD', 'HVD*', 'HVG', 'HVN', 'HVZ', 'HVZ*', 'VB', 'VB+AT', 'VB+IN', 'VB+JJ', 'VB+PPO', 'VB+RP', 'VB+TO', 'VB+VB', 'VBD', 'VBG', 'VBG+TO', 'VBN','VBN+TO', 'VBZ']\n",
    "\n",
    "    n_posTags = len(text_posTag)\n",
    "\n",
    "    adjectives = 0\n",
    "    nouns = 0\n",
    "    pronouns = 0\n",
    "    articles = 0\n",
    "    conjunctions = 0\n",
    "    numerals = 0\n",
    "    prepositions = 0\n",
    "    qualifiers = 0\n",
    "    adverbs = 0\n",
    "    foreign_words = 0\n",
    "    w_classifications = 0\n",
    "    modals = 0\n",
    "    a_determiners = 0\n",
    "    determiners = 0\n",
    "    verbs = 0\n",
    "    for tag in text_posTag:\n",
    "        if tag in adjective_tags:\n",
    "            adjectives += 1\n",
    "        elif tag in noun_tags:\n",
    "            nouns += 1\n",
    "        elif tag in pronoun_tags:\n",
    "            pronouns += 1\n",
    "        elif tag in article_tags:\n",
    "            articles += 1\n",
    "        elif tag in conjunction_tags:\n",
    "            conjunctions += 1\n",
    "        elif tag in numeral_tags:\n",
    "            numerals += 1\n",
    "        elif tag in preposition_tags:\n",
    "            prepositions += 1\n",
    "        elif tag in qualifier_tags:\n",
    "            qualifiers += 1\n",
    "        elif tag in adverb_tags:\n",
    "            adverbs += 1\n",
    "        elif foreign_word in tag:\n",
    "            foreign_words += 1\n",
    "        elif tag in w_classification_tags:\n",
    "            w_classifications += 1\n",
    "        elif tag in modal_tags:\n",
    "            modals += 1\n",
    "        elif tag in a_determiner_tags:\n",
    "            a_determiners += 1\n",
    "        elif tag in determiner_tags:\n",
    "            determiners += 1\n",
    "        elif tag in verb_tags:\n",
    "            verbs += 1\n",
    "\n",
    "    adjectives_ratio = adjectives/n_posTags\n",
    "    nouns_ratio = nouns/n_posTags\n",
    "    pronouns_ratio = pronouns/n_posTags\n",
    "    articles_ratio = articles/n_posTags\n",
    "    conjunctions_ratio = conjunctions/n_posTags\n",
    "    numerals_ratio = numerals/n_posTags\n",
    "    prepositions_ratio = prepositions/n_posTags\n",
    "    qualifiers_ratio = qualifiers/n_posTags\n",
    "    adverbs_ratio = adverbs/n_posTags\n",
    "    foreign_words_ratio = foreign_words/n_posTags\n",
    "    w_classifications_ratio = w_classifications/n_posTags\n",
    "    modals_ratio = modals/n_posTags\n",
    "    a_determiners_ratio = a_determiners/n_posTags\n",
    "    determiners_ratio = determiners/n_posTags\n",
    "    verbs_ratio = verbs/n_posTags\n",
    "\n",
    "    if adjectives_ratio > 1 or nouns_ratio > 1:\n",
    "        print(f'Len_posTags: {n_posTags} --> Adjectives: {adjectives}, --> Nouns: {nouns}')\n",
    "\n",
    "    # return adjectives_ratio, nouns_ratio\n",
    "    # return adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio\n",
    "    return adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio\n",
    "\n",
    "\n",
    "\n",
    "def extract_stopwords_and_ponctuation_ratio(text):\n",
    "\n",
    "    stopwords_count = 0\n",
    "    pontucation_count = 0\n",
    "\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    n_tokens = len(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in stopwords_list:\n",
    "            stopwords_count += 1\n",
    "        elif token in string.punctuation:\n",
    "            pontucation_count += 1\n",
    "\n",
    "    stopwords_ratio = stopwords_count/n_tokens\n",
    "    pontucation_ratio = pontucation_count/n_tokens\n",
    "\n",
    "    if stopwords_ratio > 1 or pontucation_ratio > 1:\n",
    "        print(f'Len_tokens: {n_tokens} --> Stopwords: {stopwords_count}, --> Ponctuation: {pontucation_count}')\n",
    "\n",
    "    return stopwords_ratio, pontucation_ratio\n",
    "\n",
    "\n",
    "def extract_average_token_quantity_per_sentence(text):\n",
    "    tokens_quantity = []\n",
    "    sentences =  nltk.sent_tokenize(text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        tokens_quantity.append(len(sentence_tokens))\n",
    "\n",
    "    average_tokens_quantity_per_sentence = sum(tokens_quantity)//len(tokens_quantity)\n",
    "\n",
    "    return average_tokens_quantity_per_sentence\n",
    "    \n",
    "    \n",
    "\n",
    "def extract_average_token_length_per_text(text):\n",
    "\n",
    "    tokens_length = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        tokens_length.append(len(token))\n",
    "\n",
    "    average_token_length = sum(tokens_length)//len(tokens_length)\n",
    "\n",
    "    return average_token_length\n",
    "\n",
    "\n",
    "def extract_sentence_quantity_per_text(text):\n",
    "    sentences_quantity =  len(nltk.sent_tokenize(text))\n",
    "\n",
    "    if sentences_quantity > 300:\n",
    "        print(f'To many sentences: {text}')\n",
    "\n",
    "    return sentences_quantity\n",
    "    \n",
    "\n",
    "\n",
    "def extract_features(text, text_posTag, label, dataset, min_word_size=1, max_word_size=43, min_n_sentences_per_text=3, max_n_sentences_per_text=116,  min_n_words_per_sentence=1, max_n_words_per_sentence=943 ):\n",
    "    \n",
    "    adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio = extract_posTag_features(text_posTag)\n",
    "    # adjectives_ratio, nouns_ratio, pronouns_ratio, articles_ratio, conjunctions_ratio, numerals_ratio, prepositions_ratio, qualifiers_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, a_determiners_ratio, determiners_ratio, verbs_ratio = extract_posTag_features(text_posTag)\n",
    "\n",
    "    stopwords_ratio, ponctuation_ratio = extract_stopwords_and_ponctuation_ratio(text)\n",
    "\n",
    "    average_token_quantity_per_sentence = extract_average_token_quantity_per_sentence(text)\n",
    "\n",
    "    average_token_length_per_text = extract_average_token_length_per_text(text)\n",
    "\n",
    "    # sentence_quantity_per_text = extract_sentence_quantity_per_text(text)\n",
    "    \n",
    "    # return [adjectives_ratio, nouns_ratio, stopwords_ratio, ponctuation_ratio, average_token_quantity_per_sentence, average_token_length_per_text]\n",
    "    return [adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio, stopwords_ratio, ponctuation_ratio, average_token_quantity_per_sentence, average_token_length_per_text]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(extract_features(train_data[400], train_data_posTag[400], train_labels[400], 'train'))\n",
    "\n",
    "X_train = np.array([extract_features(text, train_data_posTag[index], train_labels[index], 'train') for index, text in enumerate(train_data)])\n",
    "Y_train = train_labels\n",
    "\n",
    "X_test = np.array([extract_features(text, test_data_posTag[index], test_labels[index], 'test') for index, text in enumerate(test_data)])\n",
    "Y_test = test_labels\n",
    "\n",
    "X_val = np.array([extract_features(text, val_data_posTag[index], val_labels[index], 'val') for index, text in enumerate(val_data)])\n",
    "Y_val = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((870, 13), (870,), (190, 13), (190,), (186, 13), (186,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizers(data):\n",
    "    normalizers = []\n",
    "    n_features = len(data[0])\n",
    "\n",
    "    for index in range(n_features):\n",
    "        features = np.array([[feature[index]] for feature in full_dataset])\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(features)\n",
    "        normalizers.append(scaler)\n",
    "\n",
    "    return normalizers\n",
    "\n",
    "def normalize_features(data, normalizers):\n",
    "    normalized_features = []\n",
    "    for index, feature in enumerate(data):\n",
    "        normalized_features.append(normalizers[index].transform([[data[index]]])[0][0])\n",
    "\n",
    "    return normalized_features\n",
    "\n",
    "\n",
    "# define min max scaler\n",
    "full_dataset = np.vstack((X_train, X_test, X_val))\n",
    "\n",
    "feature_normalizers = normalizers(full_dataset)\n",
    "\n",
    "X_train = np.array([normalize_features(data, feature_normalizers) for data in X_train])\n",
    "X_test = np.array([normalize_features(data, feature_normalizers) for data in X_test])\n",
    "X_val = np.array([normalize_features(data, feature_normalizers) for data in X_val])\n",
    "\n",
    "\n",
    "# X_train, Y_train = shuffle(X_train, Y_train)\n",
    "# X_test, Y_test = shuffle(X_test, Y_test)\n",
    "# X_val, Y_val = shuffle(X_val, Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44308452, 0.34832337, 0.1956427 , 0.53335732, 0.61453386,\n",
       "       0.18496241, 0.24151003, 0.47051729, 0.63403149, 0.48049792,\n",
       "       0.5156211 , 0.11363636, 0.5       ])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_to_csv(data, labels, file_name, file_complement=''):\n",
    "    with open(f'features_csv/{file_name}_features{file_complement}.csv', 'w', newline='', encoding=\"utf-8\") as new_file:\n",
    "        for index, sample in enumerate(data):\n",
    "            sample = list(sample)\n",
    "            sample.append(labels[index])\n",
    "            writer = csv.writer(new_file)\n",
    "            writer.writerow(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset= [X_train, Y_train, X_test, Y_test, X_val, Y_val]\n",
    "\n",
    "# filename = 'processed_features'\n",
    "filename = 'data_pickles/processed_features_plus'\n",
    "# filename = 'processed_features_window_2'\n",
    "\n",
    "\n",
    "file = open(filename, 'wb')\n",
    "pickle.dump(processed_dataset, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extension = ''\n",
    "extension = '_more_features'\n",
    "# extension = '_window_2'\n",
    "\n",
    "save_features_to_csv(X_train, Y_train, 'train', extension)\n",
    "save_features_to_csv(X_test, Y_test, 'test', extension)\n",
    "save_features_to_csv(X_val, Y_val, 'val', extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
