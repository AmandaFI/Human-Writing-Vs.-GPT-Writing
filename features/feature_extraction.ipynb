{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Val Split - Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------ BBC ------------------------------------\n",
    "bbc_news_files = os.listdir('BBC/human_BBC_news/')\n",
    "bbc_labels = np.zeros(len(bbc_news_files))\n",
    "\n",
    "X_bbc_train_files, X_bbc_test_files, Y_bbc_train, Y_bbc_test =  train_test_split(bbc_news_files, bbc_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "X_bbc_val_files = X_bbc_test_files[:len(X_bbc_test_files)//2]\n",
    "X_bbc_test_files = X_bbc_test_files[len(X_bbc_test_files)//2:]\n",
    "Y_bbc_val = Y_bbc_test[:len(Y_bbc_test)//2]\n",
    "Y_bbc_test = Y_bbc_test[len(Y_bbc_test)//2:]\n",
    "\n",
    "# ------------------------------------ TechCrunch ------------------------------------\n",
    "techCrunch_news_files = os.listdir('TechCrunch/human_TechCrunch_news/')\n",
    "techCrunch_labels = np.zeros(len(techCrunch_news_files))\n",
    "\n",
    "X_techCrunch_train_files, X_techCrunch_test_files, Y_techCrunch_train, Y_techCrunch_test =  train_test_split(techCrunch_news_files, techCrunch_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "X_techCrunch_val_files = X_techCrunch_test_files[:len(X_techCrunch_test_files)//2]\n",
    "X_techCrunch_test_files = X_techCrunch_test_files[len(X_techCrunch_test_files)//2:]\n",
    "Y_techCrunch_val = Y_techCrunch_test[:len(Y_techCrunch_test)//2]\n",
    "Y_techCrunch_test = Y_techCrunch_test[len(Y_techCrunch_test)//2:]\n",
    "\n",
    "# ------------------------------------ TheVerge ------------------------------------\n",
    "theVerge_news_files = os.listdir('TheVerge/human_TheVerge_news/')\n",
    "theVerge_labels = np.zeros(len(theVerge_news_files))\n",
    "\n",
    "X_theVerge_train_files, X_theVerge_test_files, Y_theVerge_train, Y_theVerge_test =  train_test_split(theVerge_news_files, theVerge_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "X_theVerge_val_files = X_theVerge_test_files[:len(X_theVerge_test_files)//2]\n",
    "X_theVerge_test_files = X_theVerge_test_files[len(X_theVerge_test_files)//2:]\n",
    "Y_theVerge_val = Y_theVerge_test[:len(Y_theVerge_test)//2]\n",
    "Y_theVerge_test = Y_theVerge_test[len(Y_theVerge_test)//2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC --> train_files: 142, train_labels: 142, test_files: 31, test_labels: 31, val_files: 30, val_labels: 30\n",
      "Tech Crunch --> train_files: 176, train_labels: 176, test_files: 38, test_labels: 38, val_files: 38, val_labels: 38\n",
      "The Verge --> train_files: 117, train_labels: 117, test_files: 26, test_labels: 26, val_files: 25, val_labels: 25\n"
     ]
    }
   ],
   "source": [
    "print(f'BBC --> train_files: {len(X_bbc_train_files)}, train_labels: {len(Y_bbc_train)}, test_files: {len(X_bbc_test_files)}, test_labels: {len(Y_bbc_test)}, val_files: {len(X_bbc_val_files)}, val_labels: {len(Y_bbc_val)}')\n",
    "print(f'Tech Crunch --> train_files: {len(X_techCrunch_train_files)}, train_labels: {len(Y_techCrunch_train)}, test_files: {len(X_techCrunch_test_files)}, test_labels: {len(Y_techCrunch_test)}, val_files: {len(X_techCrunch_val_files)}, val_labels: {len(Y_techCrunch_val)}')\n",
    "print(f'The Verge --> train_files: {len(X_theVerge_train_files)}, train_labels: {len(Y_theVerge_train)}, test_files: {len(X_theVerge_test_files)}, test_labels: {len(Y_theVerge_test)}, val_files: {len(X_theVerge_val_files)}, val_labels: {len(Y_theVerge_val)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Human Written News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    content = None\n",
    "    folder = 'BBC/human_BBC_news/' if 'BBC' in file else 'TechCrunch/human_TechCrunch_news/' if 'TechCrunch' in file else 'TheVerge/human_TheVerge_news/'\n",
    "    with open(f'{folder}{file}', 'r', encoding=\"utf-8\") as news:\n",
    "        content = news.read()\n",
    "    return content\n",
    "\n",
    "X_bbc_train = np.array(list(map(load_data, X_bbc_train_files)))\n",
    "X_bbc_test = np.array(list(map(load_data, X_bbc_test_files)))\n",
    "X_bbc_val = np.array(list(map(load_data, X_bbc_val_files)))\n",
    "\n",
    "X_techCrunch_train = np.array(list(map(load_data, X_techCrunch_train_files)))\n",
    "X_techCrunch_test = np.array(list(map(load_data, X_techCrunch_test_files)))\n",
    "X_techCrunch_val = np.array(list(map(load_data, X_techCrunch_val_files)))\n",
    "\n",
    "X_theVerge_train = np.array(list(map(load_data, X_theVerge_train_files)))\n",
    "X_theVerge_test = np.array(list(map(load_data, X_theVerge_test_files)))\n",
    "X_theVerge_val = np.array(list(map(load_data, X_theVerge_val_files)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Human Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human train: 435, 435\n",
      "Human test: 95, 95\n",
      "Human val: 93, 93\n"
     ]
    }
   ],
   "source": [
    "X_human_train = np.hstack((X_bbc_train, X_techCrunch_train, X_theVerge_train))\n",
    "Y_human_train = np.hstack((Y_bbc_train, Y_techCrunch_train, Y_theVerge_train))\n",
    "print(f'Human train: {len(X_human_train)}, {len(Y_human_train)}')\n",
    "\n",
    "X_human_test = np.hstack((X_bbc_test, X_techCrunch_test, X_theVerge_test))\n",
    "Y_human_test = np.hstack((Y_bbc_test, Y_techCrunch_test, Y_theVerge_test))\n",
    "print(f'Human test: {len(X_human_test)}, {len(Y_human_test)}')\n",
    "\n",
    "X_human_val = np.hstack((X_bbc_val, X_techCrunch_val, X_theVerge_val))\n",
    "Y_human_val = np.hstack((Y_bbc_val, Y_techCrunch_val, Y_theVerge_val))\n",
    "print(f'Human val: {len(X_human_val)}, {len(Y_human_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------- Pickle --------------------\n",
    "\n",
    "# human_train= [X_human_train, X_human_test, X_human_val]\n",
    "\n",
    "# filename = '../dataset/news_pickles/train_test_val_Human'\n",
    "# file = open(filename, 'wb')\n",
    "# pickle.dump(human_train, file)\n",
    "# file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Human PosTags Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    content = None\n",
    "    pos_tags = None\n",
    "    folder = 'BBC/human_BBC_news/' if 'BBC' in file else 'TechCrunch/human_TechCrunch_news/' if 'TechCrunch' in file else 'TheVerge/human_TheVerge_news/'\n",
    "    with open(f'{folder}{file}', 'r', encoding=\"utf-8\") as news:\n",
    "        content = news.read()\n",
    "\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        pos_tag_tuples = nltk.pos_tag(tokens, lang='eng')\n",
    "        pos_tags = [tag[1] for tag in pos_tag_tuples]\n",
    "        \n",
    "    return pos_tags\n",
    "\n",
    "X_bbc_human_posTags_train = np.array(list(map(load_data, X_bbc_train_files)))\n",
    "X_bbc_human_posTags_test = np.array(list(map(load_data, X_bbc_test_files)))\n",
    "X_bbc_human_posTags_val = np.array(list(map(load_data, X_bbc_val_files)))\n",
    "\n",
    "X_techCrunch_human_posTags_train = np.array(list(map(load_data, X_techCrunch_train_files)))\n",
    "X_techCrunch_human_posTags_test = np.array(list(map(load_data, X_techCrunch_test_files)))\n",
    "X_techCrunch_human_posTags_val = np.array(list(map(load_data, X_techCrunch_val_files)))\n",
    "\n",
    "X_theVerge_human_posTags_train = np.array(list(map(load_data, X_theVerge_train_files)))\n",
    "X_theVerge_human_posTags_test = np.array(list(map(load_data, X_theVerge_test_files)))\n",
    "X_theVerge_human_posTags_val = np.array(list(map(load_data, X_theVerge_val_files)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human train: 435\n",
      "Human test: 95\n",
      "Human val: 93\n"
     ]
    }
   ],
   "source": [
    "X_human_train_posTags = np.hstack((X_bbc_human_posTags_train, X_techCrunch_human_posTags_train, X_theVerge_human_posTags_train))\n",
    "print(f'Human train: {len(X_human_train_posTags)}')\n",
    "\n",
    "X_human_test_posTags = np.hstack((X_bbc_human_posTags_test, X_techCrunch_human_posTags_test, X_theVerge_human_posTags_test))\n",
    "print(f'Human test: {len(X_human_test_posTags)}')\n",
    "\n",
    "X_human_val_posTags = np.hstack((X_bbc_human_posTags_val, X_techCrunch_human_posTags_val, X_theVerge_human_posTags_val))\n",
    "print(f'Human val: {len(X_human_val_posTags)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------- Pickle --------------------\n",
    "\n",
    "# human_posTags= [X_human_train_posTags, X_human_test_posTags, X_human_val_posTags]\n",
    "\n",
    "# filename = '../dataset/news_pickles/train_test_val_Human_posTags'\n",
    "# file = open(filename, 'wb')\n",
    "# pickle.dump(human_posTags, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT Written News\n",
    "#### Randomizing which set (Train, Test or Val) the human and GPT version of a same news will be placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['205_BBC_gpt.txt', '71_BBC_gpt.txt', '98_BBC_gpt.txt', '66_BBC_gpt.txt', '166_BBC_gpt.txt', '204_BBC_gpt.txt', '225_BBC_gpt.txt', '9_BBC_gpt.txt', '16_BBC_gpt.txt', '227_BBC_gpt.txt', '183_BBC_gpt.txt', '111_BBC_gpt.txt', '46_BBC_gpt.txt', '216_BBC_gpt.txt', '192_BBC_gpt.txt', '83_BBC_gpt.txt', '30_BBC_gpt.txt', '217_BBC_gpt.txt', '228_BBC_gpt.txt', '109_BBC_gpt.txt', '65_BBC_gpt.txt', '237_BBC_gpt.txt', '158_BBC_gpt.txt', '86_BBC_gpt.txt', '219_BBC_gpt.txt', '165_BBC_gpt.txt', '156_BBC_gpt.txt', '47_BBC_gpt.txt', '44_BBC_gpt.txt', '179_BBC_gpt.txt', '211_BBC_gpt.txt', '62_BBC_gpt.txt', '26_BBC_gpt.txt', '170_BBC_gpt.txt', '129_BBC_gpt.txt', '155_BBC_gpt.txt', '148_BBC_gpt.txt', '3_BBC_gpt.txt', '19_BBC_gpt.txt', '112_BBC_gpt.txt', '57_BBC_gpt.txt', '27_BBC_gpt.txt', '41_BBC_gpt.txt', '196_BBC_gpt.txt', '157_BBC_gpt.txt', '209_BBC_gpt.txt', '7_BBC_gpt.txt', '226_BBC_gpt.txt', '6_BBC_gpt.txt', '51_BBC_gpt.txt', '147_BBC_gpt.txt', '221_BBC_gpt.txt', '37_BBC_gpt.txt', '153_BBC_gpt.txt', '178_BBC_gpt.txt', '160_BBC_gpt.txt', '220_BBC_gpt.txt', '68_BBC_gpt.txt', '241_BBC_gpt.txt', '107_BBC_gpt.txt', '72_BBC_gpt.txt', '79_BBC_gpt.txt', '238_BBC_gpt.txt', '182_BBC_gpt.txt', '78_BBC_gpt.txt', '176_BBC_gpt.txt', '240_BBC_gpt.txt', '56_BBC_gpt.txt', '195_BBC_gpt.txt', '20_BBC_gpt.txt', '104_BBC_gpt.txt', '193_BBC_gpt.txt', '1_BBC_gpt.txt', '55_BBC_gpt.txt', '35_BBC_gpt.txt', '173_BBC_gpt.txt', '235_BBC_gpt.txt', '52_BBC_gpt.txt', '63_BBC_gpt.txt', '187_BBC_gpt.txt', '67_BBC_gpt.txt', '202_BBC_gpt.txt', '190_BBC_gpt.txt', '8_BBC_gpt.txt', '199_BBC_gpt.txt', '17_BBC_gpt.txt', '145_BBC_gpt.txt', '89_BBC_gpt.txt', '21_BBC_gpt.txt', '73_BBC_gpt.txt', '208_BBC_gpt.txt', '15_BBC_gpt.txt', '61_BBC_gpt.txt', '12_BBC_gpt.txt', '231_BBC_gpt.txt', '168_BBC_gpt.txt', '4_BBC_gpt.txt', '10_BBC_gpt.txt', '96_BBC_gpt.txt', '53_BBC_gpt.txt', '75_BBC_gpt.txt', '97_BBC_gpt.txt', '11_BBC_gpt.txt', '50_BBC_gpt.txt', '28_BBC_gpt.txt', '128_BBC_gpt.txt', '234_BBC_gpt.txt', '74_BBC_gpt.txt', '77_BBC_gpt.txt', '106_BBC_gpt.txt', '105_BBC_gpt.txt', '64_BBC_gpt.txt', '230_BBC_gpt.txt', '88_BBC_gpt.txt', '189_BBC_gpt.txt', '90_BBC_gpt.txt', '152_BBC_gpt.txt', '103_BBC_gpt.txt', '38_BBC_gpt.txt', '120_BBC_gpt.txt', '141_BBC_gpt.txt', '188_BBC_gpt.txt', '87_BBC_gpt.txt', '143_BBC_gpt.txt', '200_BBC_gpt.txt', '34_BBC_gpt.txt', '149_BBC_gpt.txt', '101_BBC_gpt.txt', '42_BBC_gpt.txt', '58_BBC_gpt.txt', '215_BBC_gpt.txt', '93_BBC_gpt.txt', '210_BBC_gpt.txt', '127_BBC_gpt.txt', '33_BBC_gpt.txt', '212_BBC_gpt.txt', '223_BBC_gpt.txt', '113_BBC_gpt.txt', '197_BBC_gpt.txt', '213_BBC_gpt.txt', '167_BBC_gpt.txt', '99_BBC_gpt.txt']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------ BBC ------------------------------------\n",
    "bbc_news_GPT_files = os.listdir('BBC/GPT_BBC_news/')\n",
    "bbc_GPT_labels = np.ones(len(bbc_news_GPT_files))\n",
    "\n",
    "X_bbc_GPT_train_files, X_bbc_GPT_test_files, Y_bbc_GPT_train, Y_bbc_GPT_test =  train_test_split(bbc_news_GPT_files, bbc_GPT_labels, test_size=0.3)\n",
    "\n",
    "print(X_bbc_GPT_train_files)\n",
    "\n",
    "X_bbc_GPT_val_files = X_bbc_GPT_test_files[:len(X_bbc_GPT_test_files)//2]\n",
    "X_bbc_GPT_test_files = X_bbc_GPT_test_files[len(X_bbc_GPT_test_files)//2:]\n",
    "Y_bbc_GPT_val = Y_bbc_GPT_test[:len(Y_bbc_GPT_test)//2]\n",
    "Y_bbc_GPT_test = Y_bbc_GPT_test[len(Y_bbc_GPT_test)//2:]\n",
    "\n",
    "# ------------------------------------ TechCrunch ------------------------------------\n",
    "techCrunch_news_GPT_files = os.listdir('TechCrunch/GPT_TechCrunch_news/')\n",
    "techCrunch_GPT_labels = np.ones(len(techCrunch_news_GPT_files))\n",
    "\n",
    "X_techCrunch_GPT_train_files, X_techCrunch_GPT_test_files, Y_techCrunch_GPT_train, Y_techCrunch_GPT_test =  train_test_split(techCrunch_news_GPT_files, techCrunch_GPT_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "X_techCrunch_GPT_val_files = X_techCrunch_GPT_test_files[:len(X_techCrunch_GPT_test_files)//2]\n",
    "X_techCrunch_GPT_test_files = X_techCrunch_GPT_test_files[len(X_techCrunch_GPT_test_files)//2:]\n",
    "Y_techCrunch_GPT_val = Y_techCrunch_GPT_test[:len(Y_techCrunch_GPT_test)//2]\n",
    "Y_techCrunch_GPT_test = Y_techCrunch_GPT_test[len(Y_techCrunch_GPT_test)//2:]\n",
    "\n",
    "# ------------------------------------ TheVerge ------------------------------------\n",
    "theVerge_news_GPT_files = os.listdir('TheVerge/GPT_TheVerge_news/')\n",
    "theVerge_GPT_labels = np.ones(len(theVerge_news_GPT_files))\n",
    "\n",
    "X_theVerge_GPT_train_files, X_theVerge_GPT_test_files, Y_theVerge_GPT_train, Y_theVerge_GPT_test =  train_test_split(theVerge_news_GPT_files, theVerge_GPT_labels, test_size=0.3)\n",
    "\n",
    "\n",
    "X_theVerge_GPT_val_files = X_theVerge_GPT_test_files[:len(X_theVerge_GPT_test_files)//2]\n",
    "X_theVerge_GPT_test_files = X_theVerge_GPT_test_files[len(X_theVerge_GPT_test_files)//2:]\n",
    "Y_theVerge_GPT_val = Y_theVerge_GPT_test[:len(Y_theVerge_GPT_test)//2]\n",
    "Y_theVerge_GPT_test = Y_theVerge_GPT_test[len(Y_theVerge_GPT_test)//2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC GPT --> train_files: 142, train_labels: 142, test_files: 31, test_labels: 31, val_files: 30, val_labels: 30\n",
      "Tech Crunch GPT --> train_files: 176, train_labels: 176, test_files: 38, test_labels: 38, val_files: 38, val_labels: 38\n",
      "The Verge GPT--> train_files: 117, train_labels: 117, test_files: 26, test_labels: 26, val_files: 25, val_labels: 25\n"
     ]
    }
   ],
   "source": [
    "print(f'BBC GPT --> train_files: {len(X_bbc_GPT_train_files)}, train_labels: {len(Y_bbc_GPT_train)}, test_files: {len(X_bbc_GPT_test_files)}, test_labels: {len(Y_bbc_GPT_test)}, val_files: {len(X_bbc_GPT_val_files)}, val_labels: {len(Y_bbc_GPT_val)}')\n",
    "print(f'Tech Crunch GPT --> train_files: {len(X_techCrunch_GPT_train_files)}, train_labels: {len(Y_techCrunch_GPT_train)}, test_files: {len(X_techCrunch_GPT_test_files)}, test_labels: {len(Y_techCrunch_GPT_test)}, val_files: {len(X_techCrunch_GPT_val_files)}, val_labels: {len(Y_techCrunch_GPT_val)}')\n",
    "print(f'The Verge GPT--> train_files: {len(X_theVerge_GPT_train_files)}, train_labels: {len(Y_theVerge_GPT_train)}, test_files: {len(X_theVerge_GPT_test_files)}, test_labels: {len(Y_theVerge_GPT_test)}, val_files: {len(X_theVerge_GPT_val_files)}, val_labels: {len(Y_theVerge_GPT_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    content = None\n",
    "    folder = 'BBC/GPT_BBC_news/' if 'BBC' in file else 'TechCrunch/GPT_TechCrunch_news/' if 'TechCrunch' in file else 'TheVerge/GPT_TheVerge_news/'\n",
    "    with open(f'{folder}{file}', 'r', encoding=\"utf-8\") as news:\n",
    "        content = news.read()\n",
    "    return content\n",
    "\n",
    "X_bbc_gpt_train = np.array(list(map(load_data, X_bbc_GPT_train_files)))\n",
    "X_bbc_gpt_test = np.array(list(map(load_data, X_bbc_GPT_test_files)))\n",
    "X_bbc_gpt_val = np.array(list(map(load_data, X_bbc_GPT_val_files)))\n",
    "\n",
    "X_techCrunch_gpt_train = np.array(list(map(load_data, X_techCrunch_GPT_train_files)))\n",
    "X_techCrunch_gpt_test = np.array(list(map(load_data, X_techCrunch_GPT_test_files)))\n",
    "X_techCrunch_gpt_val = np.array(list(map(load_data, X_techCrunch_GPT_val_files)))\n",
    "\n",
    "X_theVerge_gpt_train = np.array(list(map(load_data, X_theVerge_GPT_train_files)))\n",
    "X_theVerge_gpt_test = np.array(list(map(load_data, X_theVerge_GPT_test_files)))\n",
    "X_theVerge_gpt_val = np.array(list(map(load_data, X_theVerge_GPT_val_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT train: 435, 435\n",
      "GPT test: 95, 95\n",
      "GPT val: 93, 93\n"
     ]
    }
   ],
   "source": [
    "X_gpt_train = np.hstack((X_bbc_gpt_train, X_techCrunch_gpt_train, X_theVerge_gpt_train))\n",
    "Y_gpt_train = np.ones(len(X_gpt_train))\n",
    "print(f'GPT train: {len(X_gpt_train)}, {len(Y_gpt_train)}')\n",
    "\n",
    "X_gpt_test = np.hstack((X_bbc_gpt_test, X_techCrunch_gpt_test, X_theVerge_gpt_test))\n",
    "Y_gpt_test = np.ones(len(X_gpt_test))\n",
    "print(f'GPT test: {len(X_gpt_test)}, {len(Y_gpt_test)}')\n",
    "\n",
    "X_gpt_val = np.hstack((X_bbc_gpt_val, X_techCrunch_gpt_val, X_theVerge_gpt_val))\n",
    "Y_gpt_val = np.ones(len(X_gpt_val))\n",
    "print(f'GPT val: {len(X_gpt_val)}, {len(Y_gpt_val)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------- Pickle --------------------\n",
    "\n",
    "# gpt_data= [X_gpt_train, X_gpt_test, X_gpt_val]\n",
    "\n",
    "# filename = '../dataset/news_pickles/train_test_val_GPT'\n",
    "# file = open(filename, 'wb')\n",
    "# pickle.dump(gpt_data, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GPT PosTags Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    content = None\n",
    "    pos_tags = None\n",
    "    folder = 'BBC/GPT_BBC_news/' if 'BBC' in file else 'TechCrunch/GPT_TechCrunch_news/' if 'TechCrunch' in file else 'TheVerge/GPT_TheVerge_news/'\n",
    "    with open(f'{folder}{file}', 'r', encoding=\"utf-8\") as news:\n",
    "        content = news.read()\n",
    "\n",
    "        tokens = nltk.word_tokenize(content)\n",
    "        pos_tag_tuples = nltk.pos_tag(tokens, lang='eng')\n",
    "        pos_tags = [tag[1] for tag in pos_tag_tuples]\n",
    "        \n",
    "    return pos_tags\n",
    "\n",
    "\n",
    "X_bbc_GPT_posTags_train = np.array(list(map(load_data, X_bbc_GPT_train_files)))\n",
    "X_bbc_GPT_posTags_test = np.array(list(map(load_data, X_bbc_GPT_test_files)))\n",
    "X_bbc_GPT_posTags_val = np.array(list(map(load_data, X_bbc_GPT_val_files)))\n",
    "\n",
    "X_techCrunch_GPT_posTags_train = np.array(list(map(load_data, X_techCrunch_GPT_train_files)))\n",
    "X_techCrunch_GPT_posTags_test = np.array(list(map(load_data, X_techCrunch_GPT_test_files)))\n",
    "X_techCrunch_GPT_posTags_val = np.array(list(map(load_data, X_techCrunch_GPT_val_files)))\n",
    "\n",
    "X_theVerge_GPT_posTags_train = np.array(list(map(load_data, X_theVerge_GPT_train_files)))\n",
    "X_theVerge_GPT_posTags_test = np.array(list(map(load_data, X_theVerge_GPT_test_files)))\n",
    "X_theVerge_GPT_posTags_val = np.array(list(map(load_data, X_theVerge_GPT_val_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT train posTags: 435\n",
      "GPT test posTags: 95\n",
      "GPT val posTags: 93\n"
     ]
    }
   ],
   "source": [
    "X_gpt_train_posTags = np.hstack((X_bbc_GPT_posTags_train, X_techCrunch_GPT_posTags_train, X_theVerge_GPT_posTags_train))\n",
    "print(f'GPT train posTags: {len(X_gpt_train_posTags)}')\n",
    "\n",
    "X_gpt_test_posTags = np.hstack((X_bbc_GPT_posTags_test, X_techCrunch_GPT_posTags_test, X_theVerge_GPT_posTags_test))\n",
    "print(f'GPT test posTags: {len(X_gpt_test_posTags)}')\n",
    "\n",
    "X_gpt_val_posTags = np.hstack((X_bbc_GPT_posTags_val, X_techCrunch_GPT_posTags_val, X_theVerge_GPT_posTags_val))\n",
    "print(f'GPT val posTags: {len(X_gpt_val_posTags)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------- Pickle --------------------\n",
    "\n",
    "# gpt_data_posTags= [X_gpt_train_posTags, X_gpt_test_posTags, X_gpt_val_posTags]\n",
    "\n",
    "# filename = '../dataset/news_pickles/train_test_val_GPT_posTags'\n",
    "# file = open(filename, 'wb')\n",
    "# pickle.dump(gpt_data_posTags, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Text Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(870, 870, 870, 190, 190, 190, 186, 186, 186)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = np.hstack((X_human_train, X_gpt_train))\n",
    "train_labels = np.hstack((Y_human_train, Y_gpt_train))\n",
    "\n",
    "test_data = np.hstack((X_human_test, X_gpt_test))\n",
    "test_labels = np.hstack((Y_human_test, Y_gpt_test))\n",
    "\n",
    "val_data = np.hstack((X_human_val, X_gpt_val))\n",
    "val_labels = np.hstack((Y_human_val, Y_gpt_val))\n",
    "\n",
    "train_data_posTag = np.hstack((X_human_train_posTags, X_gpt_train_posTags))\n",
    "test_data_posTag = np.hstack((X_human_test_posTags, X_gpt_test_posTags))\n",
    "val_data_posTag = np.hstack((X_human_val_posTags, X_gpt_val_posTags))\n",
    "\n",
    "len(train_data), len(train_labels), len(train_data_posTag), len(test_data), len(test_labels), len(test_data_posTag), len(val_data), len(val_labels), len(val_data_posTag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_posTag_features(text_posTag):\n",
    "    adjective_tags = ['JJ', 'JJ$', 'JJ+JJ', 'JJR', 'JJR+CS', 'JJS', 'JJT', ]\n",
    "    noun_tags = ['NN', 'NN$', 'NN+BEZ', 'NN+HVD', 'NN+HVZ', 'NN+IN', 'NN+MD', 'NN+NN', 'NNS', 'NNS$', 'NNS+MD', 'NP', 'NP$', 'NP+BEZ', 'NP+HVZ', 'NP+MD', 'NPS', 'NPS$', 'NR', 'NR$', 'NR+MD', 'NRS']\n",
    "    pronoun_tags = ['PN', 'PN$', 'PN+BEZ', 'PN+HVD', 'PN+HVZ', 'PN+MD', 'PP$$', 'PPL', 'PPLS', 'PPO', 'PPS', 'PPS+BEZ', 'PPS+HVD', 'PPS+HVZ', 'PPS+MD', 'PPSS', 'PPSS+BEM', 'PPSS+BER', 'PPSS+BEZ', 'PPSS+BEZ*', 'PPSS+HV', 'PPSS+HVD', 'PPSS+MD', 'PPSS+VB']\n",
    "    article_tags = ['AT']\n",
    "    conjunction_tags = ['CC', 'CS']\n",
    "    numeral_tags = ['CD', 'CD$', 'OD']\n",
    "    preposition_tags = ['IN', 'IN+IN', 'IN+PPO']\n",
    "    qualifier_tags = ['QL', 'QLP']\n",
    "    adverb_tags = ['RB', 'RB$', 'RB+BEZ', 'RB+CS', 'RBR', 'RBR+CS', 'RBT', 'RN', 'RP', 'RP+IN']\n",
    "    foreign_word = 'FW'\n",
    "    w_classification_tags = ['WDT', 'WDT+BER', 'WDT+BER+PP', 'WDT+BEZ', 'WDT+DO+PPS', 'WDT+DOD', 'WDT+HVZ', 'WP$', 'WPO', 'WPS', 'WPS+BEZ', 'WPS+HVD', 'WPS+HVZ', 'WPS+MD', 'WQL', 'WRB','WRB+BER', 'WRB+BEZ', 'WRB+DO', 'WRB+DOD', 'WRB+DOD*', 'WRB+DOZ', 'WRB+IN', 'WRB+MD']\n",
    "    modal_tags = ['MD', 'MD*', 'MD+HV', 'MD+PPSS', 'MD+TO']\n",
    "    a_determiner_tags = ['ABL', 'ABN', 'ABX', 'AP', 'AP$', 'AP+AP']\n",
    "    determiner_tags = ['DT', 'DT$', 'DT+BEZ', 'DT+MD', 'DTI', 'DTS', 'DTX']\n",
    "    verb_tags = ['BE', 'BED', 'BED*', 'BEDZ', 'BEDZ*', 'BEG', 'BEM', 'BEM*', 'BEN', 'BER', 'BER*', 'BEZ', 'BEZ*', 'DO', 'DO*', 'DO+PPSS', 'DOD', 'DOD*', 'DOZ', 'DOZ*', 'HV','HV*', 'HV+TO', 'HVD', 'HVD*', 'HVG', 'HVN', 'HVZ', 'HVZ*', 'VB', 'VB+AT', 'VB+IN', 'VB+JJ', 'VB+PPO', 'VB+RP', 'VB+TO', 'VB+VB', 'VBD', 'VBG', 'VBG+TO', 'VBN','VBN+TO', 'VBZ']\n",
    "\n",
    "    n_posTags = len(text_posTag)\n",
    "\n",
    "    adjectives = 0\n",
    "    nouns = 0\n",
    "    pronouns = 0\n",
    "    articles = 0\n",
    "    conjunctions = 0\n",
    "    numerals = 0\n",
    "    prepositions = 0\n",
    "    qualifiers = 0\n",
    "    adverbs = 0\n",
    "    foreign_words = 0\n",
    "    w_classifications = 0\n",
    "    modals = 0\n",
    "    a_determiners = 0\n",
    "    determiners = 0\n",
    "    verbs = 0\n",
    "    for tag in text_posTag:\n",
    "        if tag in adjective_tags:\n",
    "            adjectives += 1\n",
    "        elif tag in noun_tags:\n",
    "            nouns += 1\n",
    "        elif tag in pronoun_tags:\n",
    "            pronouns += 1\n",
    "        elif tag in article_tags:\n",
    "            articles += 1\n",
    "        elif tag in conjunction_tags:\n",
    "            conjunctions += 1\n",
    "        elif tag in numeral_tags:\n",
    "            numerals += 1\n",
    "        elif tag in preposition_tags:\n",
    "            prepositions += 1\n",
    "        elif tag in qualifier_tags:\n",
    "            qualifiers += 1\n",
    "        elif tag in adverb_tags:\n",
    "            adverbs += 1\n",
    "        elif foreign_word in tag:\n",
    "            foreign_words += 1\n",
    "        elif tag in w_classification_tags:\n",
    "            w_classifications += 1\n",
    "        elif tag in modal_tags:\n",
    "            modals += 1\n",
    "        elif tag in a_determiner_tags:\n",
    "            a_determiners += 1\n",
    "        elif tag in determiner_tags:\n",
    "            determiners += 1\n",
    "        elif tag in verb_tags:\n",
    "            verbs += 1\n",
    "\n",
    "    adjectives_ratio = adjectives/n_posTags\n",
    "    nouns_ratio = nouns/n_posTags\n",
    "    pronouns_ratio = pronouns/n_posTags\n",
    "    articles_ratio = articles/n_posTags\n",
    "    conjunctions_ratio = conjunctions/n_posTags\n",
    "    numerals_ratio = numerals/n_posTags\n",
    "    prepositions_ratio = prepositions/n_posTags\n",
    "    qualifiers_ratio = qualifiers/n_posTags\n",
    "    adverbs_ratio = adverbs/n_posTags\n",
    "    foreign_words_ratio = foreign_words/n_posTags\n",
    "    w_classifications_ratio = w_classifications/n_posTags\n",
    "    modals_ratio = modals/n_posTags\n",
    "    a_determiners_ratio = a_determiners/n_posTags\n",
    "    determiners_ratio = determiners/n_posTags\n",
    "    verbs_ratio = verbs/n_posTags\n",
    "\n",
    "    if adjectives_ratio > 1 or nouns_ratio > 1:\n",
    "        print(f'Len_posTags: {n_posTags} --> Adjectives: {adjectives}, --> Nouns: {nouns}')\n",
    "\n",
    "    return adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio\n",
    "\n",
    "\n",
    "def extract_stopwords_and_ponctuation_ratio(text):\n",
    "\n",
    "    stopwords_count = 0\n",
    "    pontucation_count = 0\n",
    "\n",
    "    stopwords_list = stopwords.words(\"english\")\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    n_tokens = len(tokens)\n",
    "\n",
    "    for token in tokens:\n",
    "        if token in stopwords_list:\n",
    "            stopwords_count += 1\n",
    "        elif token in string.punctuation:\n",
    "            pontucation_count += 1\n",
    "\n",
    "    stopwords_ratio = stopwords_count/n_tokens\n",
    "    pontucation_ratio = pontucation_count/n_tokens\n",
    "\n",
    "    if stopwords_ratio > 1 or pontucation_ratio > 1:\n",
    "        print(f'Len_tokens: {n_tokens} --> Stopwords: {stopwords_count}, --> Ponctuation: {pontucation_count}')\n",
    "\n",
    "    return stopwords_ratio, pontucation_ratio\n",
    "\n",
    "\n",
    "def extract_average_token_quantity_per_sentence(text):\n",
    "    tokens_quantity = []\n",
    "    sentences =  nltk.sent_tokenize(text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = nltk.word_tokenize(sentence)\n",
    "        tokens_quantity.append(len(sentence_tokens))\n",
    "\n",
    "    average_tokens_quantity_per_sentence = sum(tokens_quantity)//len(tokens_quantity)\n",
    "\n",
    "    return average_tokens_quantity_per_sentence\n",
    "    \n",
    "    \n",
    "\n",
    "def extract_average_token_length_per_text(text):\n",
    "\n",
    "    tokens_length = []\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    for token in tokens:\n",
    "        tokens_length.append(len(token))\n",
    "\n",
    "    average_token_length = sum(tokens_length)//len(tokens_length)\n",
    "\n",
    "    return average_token_length\n",
    "\n",
    "\n",
    "def extract_sentence_quantity_per_text(text):\n",
    "    sentences_quantity =  len(nltk.sent_tokenize(text))\n",
    "\n",
    "    if sentences_quantity > 300:\n",
    "        print(f'To many sentences: {text}')\n",
    "\n",
    "    return sentences_quantity\n",
    "    \n",
    "\n",
    "\n",
    "def extract_features(text, text_posTag, label, dataset, min_word_size=1, max_word_size=43, min_n_sentences_per_text=3, max_n_sentences_per_text=116,  min_n_words_per_sentence=1, max_n_words_per_sentence=943 ):\n",
    "    \n",
    "    adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio = extract_posTag_features(text_posTag)\n",
    "\n",
    "    stopwords_ratio, ponctuation_ratio = extract_stopwords_and_ponctuation_ratio(text)\n",
    "\n",
    "    average_token_quantity_per_sentence = extract_average_token_quantity_per_sentence(text)\n",
    "\n",
    "    average_token_length_per_text = extract_average_token_length_per_text(text)\n",
    "\n",
    "    \n",
    "    return [adjectives_ratio, nouns_ratio, conjunctions_ratio, prepositions_ratio, adverbs_ratio, w_classifications_ratio, modals_ratio, determiners_ratio, verbs_ratio, stopwords_ratio, ponctuation_ratio, average_token_quantity_per_sentence, average_token_length_per_text]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([extract_features(text, train_data_posTag[index], train_labels[index], 'train') for index, text in enumerate(train_data)])\n",
    "Y_train = train_labels\n",
    "\n",
    "X_test = np.array([extract_features(text, test_data_posTag[index], test_labels[index], 'test') for index, text in enumerate(test_data)])\n",
    "Y_test = test_labels\n",
    "\n",
    "X_val = np.array([extract_features(text, val_data_posTag[index], val_labels[index], 'val') for index, text in enumerate(val_data)])\n",
    "Y_val = val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((870, 13), (870,), (190, 13), (190,), (186, 13), (186,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, X_val.shape, Y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizers(data):\n",
    "    normalizers = []\n",
    "    n_features = len(data[0])\n",
    "\n",
    "    for index in range(n_features):\n",
    "        features = np.array([[feature[index]] for feature in full_dataset])\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(features)\n",
    "        normalizers.append(scaler)\n",
    "\n",
    "    return normalizers\n",
    "\n",
    "def normalize_features(data, normalizers):\n",
    "    normalized_features = []\n",
    "    for index, feature in enumerate(data):\n",
    "        normalized_features.append(normalizers[index].transform([[data[index]]])[0][0])\n",
    "\n",
    "    return normalized_features\n",
    "\n",
    "\n",
    "full_dataset = np.vstack((X_train, X_test, X_val))\n",
    "\n",
    "feature_normalizers = normalizers(full_dataset)\n",
    "\n",
    "X_train = np.array([normalize_features(data, feature_normalizers) for data in X_train])\n",
    "X_test = np.array([normalize_features(data, feature_normalizers) for data in X_test])\n",
    "X_val = np.array([normalize_features(data, feature_normalizers) for data in X_val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Extracted Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features_to_csv(data, labels, file_name, file_complement=''):\n",
    "    with open(f'features_csv/{file_name}_features{file_complement}.csv', 'w', newline='', encoding=\"utf-8\") as new_file:\n",
    "        for index, sample in enumerate(data):\n",
    "            sample = list(sample)\n",
    "            sample.append(labels[index])\n",
    "            writer = csv.writer(new_file)\n",
    "            writer.writerow(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------- Pickle --------------------\n",
    "\n",
    "# processed_dataset= [X_train, Y_train, X_test, Y_test, X_val, Y_val]\n",
    "\n",
    "# filename = 'processed_features'\n",
    "\n",
    "# file = open(filename, 'wb')\n",
    "# pickle.dump(processed_dataset, file)\n",
    "# file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -------------------- CSV --------------------\n",
    "# extension = '_features'\n",
    "\n",
    "# save_features_to_csv(X_train, Y_train, 'extracted_features/train', extension)\n",
    "# save_features_to_csv(X_test, Y_test, 'extracted_features/test', extension)\n",
    "# save_features_to_csv(X_val, Y_val, 'extracted_features/val', extension)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
